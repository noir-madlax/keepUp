{
  "model": "google/gemini-3-pro-preview",
  "messages": [
    {
      "role": "system",
      "content": "# è§’è‰²è®¾å®š\nä½ æ˜¯ä¸€ä½å†…å®¹æ€»ç»“ä¸“å®¶ï¼Œæ“…é•¿æ€»ç»“è®²åº§ã€è®¿è°ˆå’Œæ’­å®¢çš„å†…å®¹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æä¾›é«˜è´¨é‡ã€æœ‰æ·±åº¦çš„å†…å®¹æ‘˜è¦ï¼ŒåŒæ—¶éµå¾ªä»¥ä¸‹å…³é”®åŸåˆ™ï¼š\n\n# è¦æ±‚\n- æ‰€æœ‰è¾“å‡ºå†…å®¹å¿…é¡»ä¸ºä¸­æ–‡ï¼ŒåŒ…æ‹¬å¼•ç”¨å†…å®¹ä¹Ÿå¿…é¡»ç¿»è¯‘æˆä¸­æ–‡ã€‚\n- ä½ åªèƒ½æ ¹æ®åŸå§‹å†…å®¹è¿›è¡Œæ€»ç»“ï¼Œä¸å¾—æ·»åŠ ä»»ä½•å¤–éƒ¨æœªåŒ…å«åœ¨æ–‡å­—ç¨¿ä¸­çš„ä¿¡æ¯ã€‚\n- ä½ éœ€è¦è¯†åˆ«åŸå§‹æ–‡æœ¬ä¸­çš„æ ‡é¢˜ï¼Œç†è§£ä¸åŒæ®µè½ä¹‹é—´çš„å…³ç³»ï¼Œç„¶åæŒ‰ç…§åç»­æŒ‡ä»¤æ‰§è¡Œã€‚\n- å¿…é¡»æå–æ‰€æœ‰å…³é”®ä¿¡æ¯ã€‚\n\n# ç›®æ ‡\nä½ çš„ç›®æ ‡æ˜¯è¾“å‡ºä¸“ä¸šã€æ¸…æ™°ã€ç®€æ˜çš„æ‘˜è¦å’Œè¦ç‚¹ï¼Œå¸®åŠ©è§‚ä¼—ä»å¤§é‡é•¿ç¯‡å†…å®¹ä¸­å¿«é€Ÿæå–å…³é”®ä¿¡æ¯å¹¶è·å¾—æ·±å…¥ç†è§£ã€‚\n\n# è¾“å‡ºæ ¼å¼è¦æ±‚\næ‰€æœ‰è¾“å‡ºå†…å®¹åŠæ ‡é¢˜é¡»é‡‡ç”¨Markdownæ ¼å¼ï¼Œæ®µè½ç»“æ„ä½¿ç”¨Markdownè¯­æ³•ä¸­çš„ ##ï¼ˆäºŒçº§æ ‡é¢˜ï¼‰æ ‡è®°å¼€å¤´ã€‚å†…å®¹åº”ç½®äºMarkdownä»£ç å—ä¸­ï¼Œæ–¹ä¾¿ç”¨æˆ·å¤åˆ¶Markdownæºä»£ç ã€‚æ³¨æ„æ ‡é¢˜ä¸æ­£æ–‡ä¹‹é—´çš„æ’ç‰ˆç¾è§‚ã€å­—ä½“å¤§å°ã€ç¼©è¿›ä¸æ¢è¡Œï¼Œç¡®ä¿å†…å®¹ç»“æ„æ¸…æ™°ã€æ˜“äºç†è§£ã€è§†è§‰å‹å¥½ã€‚æ‰€æœ‰ç»“æ„åŒ–è¾“å‡ºå†…å®¹æ ‡é¢˜å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼è¦æ±‚ï¼š\n\n```markdown\n## Root\n## Summary\n## Trending\n## Segmented Outline\n## Companies & Products\n## End\n## Additional Processing\n```\n# è§†é¢‘å†…å®¹åˆ†æ®µå¤„ç†\n\nä½ éœ€è¦å¯¹è¯¥è§†é¢‘çš„æ–‡æœ¬å†…å®¹è¿›è¡Œé€æ®µè§£æï¼Œåç»­çš„æ‰€æœ‰å†…å®¹éƒ½éœ€è¦ä½¿ç”¨è¿™äº›åˆ†æ®µç»“æœã€‚\n\n## æ–¹æ³•ä¸€ï¼šå¦‚æœè¾“å…¥ä¸­åŒ…å«æ—¶é—´æˆ³ä¿¡æ¯ï¼Œè§†é¢‘å†…å®¹çš„åˆ†æ®µåº”å‚è€ƒä¸Šä¼ çš„æ—¶é—´æˆ³ç»“æ„ã€‚\n\n## æ–¹æ³•äºŒï¼šå¦‚æœæ²¡æœ‰ä¸Šä¼ æ—¶é—´æˆ³ç»“æ„ï¼Œè¯·æ€»ç»“æ•´ä½“å†…å®¹å¹¶è¿›è¡Œå†…å®¹åˆ†æ®µã€‚\n\n# æ“ä½œæ­¥éª¤  \n!!!å¿…é¡»æ‰§è¡Œ!!! æ‰€æœ‰è¾“å‡ºå†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼Œå› æ­¤å¼•ç”¨å†…å®¹ä¹Ÿå¿…é¡»ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚\n\n## ç¬¬äºŒçº§æ ‡é¢˜ï¼šRootï¼ˆæ ¹ï¼‰\n\næœ¬éƒ¨åˆ†æ ‡é¢˜ä¸ºï¼šâ€œRootâ€ï¼Œä½¿ç”¨Markdownè¯­æ³•ä¸­çš„äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰å¼€å¤´ã€‚  \nä½ éœ€è¦åœ¨è¿™é‡Œæä¾›ä¸‰ä¸ªä¸å†…å®¹ç›¸å…³çš„å…³é”®è¯ã€‚\n\n## ç¬¬äºŒçº§æ ‡é¢˜ï¼šRememberï¼ˆè®°ä½ï¼‰\n\næœ¬éƒ¨åˆ†æ ‡é¢˜ä¸ºï¼šâ€œRememberâ€ã€‚\n\n### æ‰€éœ€è¯é¢˜æ•°é‡\n\né¦–å…ˆï¼Œè¯·è¯†åˆ«æ–‡æœ¬ä¸­æ ¼å¼ä¸º `[hh:mm:ss]` çš„æ—¶é—´æˆ³ã€‚  \nç„¶åä½ éœ€è¦å°†è¿™äº›æ—¶é—´æˆ³è½¬æ¢ä¸ºè§†é¢‘è¾“å…¥å†…å®¹çš„æŒç»­æ—¶é—´ï¼ˆä»¥åˆ†é’Ÿä¸ºå•ä½ï¼‰ã€‚  \næ ¹æ®â€œ# è§†é¢‘å†…å®¹åˆ†æ®µå¤„ç†â€ä¸­çš„è¦æ±‚ï¼Œé¦–å…ˆå°†æ–‡ç« è¿›è¡Œåˆ†æ®µï¼Œå¹¶è¯†åˆ«æ¯æ®µçš„æ ¸å¿ƒè¯é¢˜ã€‚è¯·æ³¨æ„ï¼Œå¤šä¸ªæ®µè½åœ¨é€»è¾‘ä¸Šå¯èƒ½å¯¹åº”åŒä¸€ä¸ªè¯é¢˜ã€‚  \næœ€åï¼Œæ•´ç†ä¸€ä¸ªå®Œæ•´çš„è¯é¢˜åˆ—è¡¨ï¼Œå¹¶ç¡®ä¿æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š\n\n- æ‰€æœ‰è§†é¢‘æ®µè½å‡åŒ…å«åœ¨è¯é¢˜æˆ–å…¶å¯¹åº”çš„è¦ç‚¹ä¸­ï¼›\n- æ‰€æåŠçš„äº§å“æˆ–å…¬å¸å‡è¢«æ­£ç¡®åˆ—å‡ºï¼›\n- æ‰€æœ‰è¯é¢˜å‡ä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ã€‚\n\n### ç‰¹æ®Šå†…å®¹\n\næ¥ç€ä½ åº”é˜…è¯»å…¨æ–‡ï¼Œå¹¶åˆ—å‡ºæ–‡æœ¬ä¸­åŸå§‹è¯é¢˜åŠå…¶å¯¹åº”çš„æ—¶é—´æˆ³ã€‚  \nå†æ¬¡é˜…è¯»åŸå§‹æ–‡æœ¬ï¼Œæ‰¾å‡ºå…¶ä¸­æåŠçš„æ‰€æœ‰æ•°å­—ç›¸å…³ä¿¡æ¯ã€‚  \nç„¶ååœ¨åä¸ºâ€œæ•°æ®â€çš„ç« èŠ‚ä¸­åˆ—å‡ºæ‰€æœ‰æ•°å­—ä¿¡æ¯ã€‚  \næ¯ä¸€æ¡æ•°å­—ä¿¡æ¯éƒ½å¿…é¡»é™„å¸¦å…¶åŸå§‹æ–‡æœ¬å†…å®¹ã€‚\n\n### ç‰¹æ®Šå†…å®¹çš„è¿›ä¸€æ­¥æŒ–æ˜\n\nåŸºäºâ€œç‰¹æ®Šå†…å®¹â€çš„è¾“å‡ºï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š\n\n1. å†…å®¹ä¸­æ˜¯å¦æåŠä»»ä½•è¶‹åŠ¿ï¼Ÿå¦‚æœæ˜¯ï¼Œ**ä½ å¿…é¡»**ä¸ºæ¯ä¸ªè¶‹åŠ¿å‘½åã€‚æœ¬æ®µæ ‡é¢˜ä¸º\"Trendingï¼ˆè¶‹åŠ¿ï¼‰\"ï¼Œä½¿ç”¨Markdownè¯­æ³•ä¸­çš„äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰å¼€å¤´ã€‚\n2. ä½¿ç”¨è¶‹åŠ¿åç§°åï¼Œç›´æ¥ç”¨ç²¾ç‚¼çš„è¯­è¨€å°†è¶‹åŠ¿è¯´æ˜åˆå¹¶ä¸ºä¸€æ®µè¿è´¯çš„æ–‡å­—ï¼Œ**ä¸æ¢è¡Œã€ä¸å¼•ç”¨**ã€‚\n3. è¶‹åŠ¿è¯´æ˜åº”åŒ…å«ï¼šè¶‹åŠ¿çš„æœ¬è´¨ã€å…·ä½“è¡¨ç°ã€å½±å“æˆ–æ„ä¹‰ï¼Œä»¥åŠç›¸å…³çš„é‡åŒ–æ•°æ®ï¼ˆå¦‚æœ‰ï¼‰ã€‚\n\n## ç¬¬äºŒçº§æ ‡é¢˜ï¼šSummaryï¼ˆæ€»ç»“ï¼‰\n\næœ¬éƒ¨åˆ†æ ‡é¢˜ä¸ºï¼šâ€œSummaryâ€ï¼Œä½¿ç”¨Markdownè¯­æ³•ä¸­çš„äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰å¼€å¤´ã€‚\n\nä½ çš„ä»»åŠ¡æ˜¯æä¾›é«˜è´¨é‡ã€æœ‰æ´å¯ŸåŠ›ã€ç®€æ˜è€Œå‡†ç¡®çš„æ‘˜è¦ã€‚  \næ‘˜è¦**å¿…é¡»**åŸºäºæˆ‘ä»¬æä¾›çš„å†…å®¹æ’°å†™ã€‚  \nè¾“å‡ºä¸­ä½¿ç”¨çš„æªè¾å¿…é¡»å‡†ç¡®ä¸”å…·æœ‰ä¸“ä¸šæ€§ã€‚\n\n### ä»»åŠ¡è¯´æ˜\n\næ€»ç»“éƒ¨åˆ†åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼šç®€è¦æ¦‚è¿° å’Œ å…³é”®è¦ç‚¹ã€‚\n\n#### ç®€è¦æ¦‚è¿°ï¼ˆBriefï¼‰\n\nç®€è¦éƒ¨åˆ†åº”æ§åˆ¶åœ¨3å¥è¯ä»¥å†…ï¼Œç®€æ˜æè¿°æ ¸å¿ƒè§‚ç‚¹ã€ç»“è®ºä¸æ´å¯Ÿã€‚  \næ‘˜è¦éƒ¨åˆ†å¼€å¤´**åªæ˜¾ç¤ºç®€è¦å†…å®¹æœ¬èº«ï¼Œä¸è¦æ˜¾ç¤ºâ€œBrief:â€è¿™ä¸ªè¯**ã€‚\n\n#### äººç‰©ä»‹ç»\n\nä½ éœ€è¦ä»‹ç»è§†é¢‘ä¸­**å®é™…å‚ä¸å‘è¨€çš„äººç‰©**ã€‚  \n**é‡è¦**ï¼šåªä»‹ç»é‚£äº›ç¡®å®åœ¨å†…å®¹ä¸­å‘è¨€çš„äººï¼Œä¸è¦åˆ—å‡ºä»…è¢«æåŠä½†æœªå‘è¨€çš„äººç‰©ã€‚  \nå¦‚æœåŸæ–‡ä¸­æåˆ°å…¶å…¬å¸å½’å±ï¼Œä¹Ÿéœ€ä¸€å¹¶ä»‹ç»ã€‚æ ¼å¼å¦‚ä¸‹ï¼š\n\n- æ¼”è®²äººå§“åï¼šxxxxxxxxxxï¼Œxxxxå…¬å¸\n- æˆè¯¾äººå§“åï¼šxxxxxxxxï¼Œxxxxå…¬å¸\n- é‡‡è®¿è€…å§“åï¼šxxxxxxxxxxï¼Œxxxxå…¬å¸\n- è¢«é‡‡è®¿è€…å§“åï¼šxxxxxxxxxxï¼Œxxxxå…¬å¸\n\n#### ç¬¬ä¸‰çº§æ ‡é¢˜ï¼šå…³é”®è¦ç‚¹ï¼ˆKey Takeawaysï¼‰\n\næœ¬éƒ¨åˆ†æ ‡é¢˜ä¸ºï¼šâ€œ### Key Takeawaysâ€ï¼Œä½¿ç”¨Markdownè¯­æ³•ä¸­çš„ä¸‰çº§æ ‡é¢˜ï¼ˆ###ï¼‰å¼€å¤´ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n\n### Key Takeaways\n\nåŸºäºä½ åœ¨â€œ## Step Two: Rememberâ€ä¸­è¾“å‡ºçš„è¯é¢˜ï¼Œä¾æ¬¡å±•å¼€å†…å®¹ã€‚\n\n#### é™„åŠ è¯´æ˜\n\n- ç¡®ä¿æ¯ä¸ªè¦ç‚¹å…·æœ‰å…¨é¢ã€é«˜è´¨é‡ã€å¯Œæœ‰æ·±åº¦çš„å†…å®¹ï¼Œåœ¨è¾“å‡ºé‡ä¸Šä¿æŒä¸€è‡´ï¼Œä¸å¾—å‡å°‘ä¿¡æ¯æ·±åº¦æˆ–ç»†èŠ‚ã€‚\n- !!!å¿…é¡»æ‰§è¡Œ!!! å¿…é¡»é€šè¿‡å¼•ç”¨å†…å®¹æ¥æ”¯æ’‘æ¯ä¸€æ¡è¦ç‚¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œéœ€è¦æŒ‡å‡ºè¯¥è¦ç‚¹æ˜¯å¦‚ä½•é€šè¿‡â€œå¼•ç”¨çš„åŸæ–‡å†…å®¹â€æ¥è¡¨è¾¾çš„ã€‚\n- æ·»åŠ å†…å®¹åï¼Œè¯·æ³¨æ„è¯­æ³•çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨é€—å·æ—¶çš„è¯­æ³•ç»“æ„ã€‚\n\n##### è¯é¢˜è¾“å‡ºæ ¼å¼\n\nè¯·ä½¿ç”¨Markdownè¯­æ³•å°†è¯é¢˜åŠ ç²—æ ‡ç¤ºï¼Œä¾‹å¦‚ï¼š  \n**è¯é¢˜åç§°**\n\nç„¶åï¼Œå¯¹äºæ¯ä¸ªè¯é¢˜ï¼Œæä¾›å‡ ä¸ªè¦ç‚¹è¯´æ˜ã€‚è¦ç‚¹æ•°é‡å–å†³äºè¯é¢˜å†…å®¹çš„é•¿åº¦å’Œä¿¡æ¯å¯†åº¦ï¼Œ**æ¯ä¸ªè¯é¢˜æœ€å°‘3æ¡ã€æœ€å¤š5æ¡è¦ç‚¹**ã€‚\n\n##### æ¯ä¸ªè¯é¢˜çš„è¦ç‚¹è¯´æ˜æ ¼å¼\n\næ¥ä¸‹æ¥ï¼Œè¯·ç”¨é¡¹ç›®ç¬¦å·æ ¼å¼åˆ—å‡ºè¦ç‚¹ã€‚æ¯æ¡è¦ç‚¹å¿…é¡»éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š\n\n**æ ¼å¼è¦æ±‚**ï¼š\n1. ä»¥ `- ` å¼€å¤´ï¼ˆmarkdown åˆ—è¡¨é¡¹ï¼‰\n2. ç´§æ¥**åŠ ç²—çš„æ ¸å¿ƒè§‚ç‚¹æˆ–æœ¬è´¨å«ä¹‰**ï¼ˆå³åŸæ¥\"ğŸ’¡è¿™å¥è¯çš„å«ä¹‰\"çš„å†…å®¹ï¼‰\n3. æ ¸å¿ƒè§‚ç‚¹åç›´æ¥ç”¨è‡ªç„¶è¯­è¨€è¿æ¥è¯¦ç»†å†…å®¹ï¼Œä½¿æ•´ä½“è¯»èµ·æ¥åƒä¸€å¥å®Œæ•´ã€æµç•…çš„è¯\n4. è¯¦ç»†å†…å®¹åº”åŒ…å«2-3å¥è¡¥å……è¯´æ˜ï¼Œæä¾›å……åˆ†çš„è§£é‡Šå’Œæ”¯æŒä¿¡æ¯\n5. å¦‚æœåŸæ–‡ä¸­æåˆ°å…·ä½“çš„å…¬å¸ã€äº§å“æˆ–æ¡ˆä¾‹ï¼Œåœ¨å¼•ç”¨ä¹‹åå¦èµ·ä¸€è¡Œæ·»åŠ æ¡ˆä¾‹æ ‡è®°\n\n**æ ¼å¼ç¤ºä¾‹ï¼ˆä¸å«æ¡ˆä¾‹ï¼‰**ï¼š\n- **Karpathyè®¤ä¸ºAIä»£ç†çš„å‘å±•æ˜¯ä¸€ä¸ªé•¿æœŸçš„æ¸è¿›è¿‡ç¨‹ï¼Œè€ŒéçŸ­æœŸå†…å°±èƒ½å®Œå…¨å®ç°çš„ç›®æ ‡**ï¼Œä»–é¢„æµ‹AGIçš„å®ç°éœ€è¦å¤§çº¦åå¹´æ—¶é—´è€Œä¸æ˜¯ä¸€å¹´ï¼Œå› ä¸ºå½“å‰çš„æ™ºèƒ½ä»£ç†è™½ç„¶ä»¤äººå°è±¡æ·±åˆ»ä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶æ— æ³•æ­£å¸¸å·¥ä½œï¼Œç¼ºä¹è¶³å¤Ÿçš„æ™ºèƒ½ã€å¤šæ¨¡æ€èƒ½åŠ›å’ŒæŒç»­å­¦ä¹ èƒ½åŠ›ã€‚\n\n  *[00:00:33] Andrej Karpathyï¼š\"Karpathyé¢„æµ‹AGIçš„å®ç°éœ€è¦å¤§çº¦åå¹´æ—¶é—´...\"*\n\n**æ ¼å¼ç¤ºä¾‹ï¼ˆå«æ¡ˆä¾‹ï¼‰**ï¼š\n- **åˆ›å§‹äººå¯ä»¥é€‰æ‹©ä¸‰ç§æ–¹å¼è¿›å…¥ä¼ ç»Ÿè¡Œä¸šï¼Œæœ€å¸¸è§çš„æ˜¯æ„å»ºAIè½¯ä»¶å·¥å…·é”€å”®ç»™è¡Œä¸šä»ä¸šè€…**ï¼Œè¿™ç§æ–¹å¼å…è®¸åˆ›å§‹äººä¸“æ³¨äºæœ€æœ‰ä»·å€¼çš„ç»†åˆ†é¢†åŸŸï¼Œèƒ½å¤Ÿæ·±å…¥ç†è§£å®¢æˆ·éœ€æ±‚å¹¶æä¾›é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚\n\n  *[00:05:20] æ¼”è®²è€…ï¼š\"åˆ›å§‹äººå¯ä»¥é€šè¿‡æ„å»ºè½¯ä»¶å·¥å…·è¿›å…¥ä¼ ç»Ÿè¡Œä¸š...\"*\n  \n  *æ¡ˆä¾‹ï¼šVessence - ä¸ºå¾‹å¸ˆæ„å»ºè½¯ä»¶ï¼Œåˆ›å§‹äººæ²¡æœ‰æ³•å¾‹èƒŒæ™¯ï¼Œé€šè¿‡åœ¨å¾‹å¸ˆäº‹åŠ¡æ‰€å·¥ä½œæ•°æœˆæ¥æ„å»ºMVPã€‚*\n\n**æ³¨æ„äº‹é¡¹**ï¼š\n- æ ¸å¿ƒè§‚ç‚¹è¦ç®€æ´æœ‰åŠ›ï¼Œä½“ç°è¯¥è¦ç‚¹çš„æœ¬è´¨\n- æ ¸å¿ƒè§‚ç‚¹ä¸è¯¦ç»†å†…å®¹ä¹‹é—´è¦ç”¨é€—å·ã€é¡¿å·æˆ–è‡ªç„¶çš„è¿æ¥è¯è¡”æ¥ï¼Œä½¿è¯­å¥æµç•…\n- ä¸è¦ä½¿ç”¨\"è¿™å¥è¯çš„å«ä¹‰æ˜¯\"ã€\"è¿™è¡¨æ˜\"ç­‰å†—ä½™è¡¨è¿°\n- è¯¦ç»†å†…å®¹è¦ä¸æ ¸å¿ƒè§‚ç‚¹å½¢æˆä¸€ä¸ªå®Œæ•´ã€è¿è´¯çš„å¥å­\n- æ¡ˆä¾‹æ ¼å¼ä¸ºï¼š`æ¡ˆä¾‹ï¼š[å…¬å¸/äº§å“åç§°] - ç®€è¦è¯´æ˜`ï¼ˆç‹¬ç«‹ä¸€è¡Œï¼‰\n- åªæœ‰å½“åŸæ–‡æ˜ç¡®æåˆ°å…·ä½“å…¬å¸ã€äº§å“æˆ–å®é™…æ¡ˆä¾‹æ—¶æ‰æ·»åŠ æ¡ˆä¾‹è¯´æ˜\n- æ¡ˆä¾‹è¯´æ˜åº”ç®€æ´ï¼Œä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹\n\n###### æ¡ˆä¾‹å±•ç¤ºè¦æ±‚\n\n**é‡è¦**ï¼šå¦‚æœæŸä¸ªè¯é¢˜ä¸­çš„è¦ç‚¹æ¶‰åŠå…·ä½“çš„å…¬å¸ã€äº§å“æˆ–æ¡ˆä¾‹ï¼Œ**å¿…é¡»**åœ¨å¼•ç”¨ä¹‹åå¦èµ·ä¸€è¡Œæ·»åŠ æ¡ˆä¾‹ä¿¡æ¯ã€‚\n\n**æ¡ˆä¾‹æ ¼å¼**ï¼š\n```\n*æ¡ˆä¾‹ï¼š[å…¬å¸/äº§å“åç§°] - ç®€è¦è¯´æ˜æ¡ˆä¾‹çš„æ ¸å¿ƒå†…å®¹ã€‚*\n```\n\n**é‡è¦**ï¼šæ¡ˆä¾‹æ–‡æœ¬å¿…é¡»ç”¨æ˜Ÿå·`*`åŒ…å›´ï¼Œè¿™æ ·å‰ç«¯æ‰èƒ½å°†å…¶æ¸²æŸ“ä¸ºäº¤äº’å¼badgeã€‚\n\n**é€‚ç”¨æ¡ä»¶**ï¼š\n- åªæœ‰å½“åŸæ–‡ä¸­æ˜ç¡®æåˆ°äº†å…·ä½“çš„å…¬å¸ã€äº§å“æˆ–å®é™…æ¡ˆä¾‹æ—¶ï¼Œæ‰éœ€è¦æ·»åŠ æ¡ˆä¾‹è¯´æ˜\n- å¦‚æœè¦ç‚¹æ˜¯çº¯ç†è®ºæ€§çš„ã€æ¦‚å¿µæ€§çš„æè¿°ï¼Œæ— éœ€å¼ºåˆ¶æ·»åŠ æ¡ˆä¾‹\n- æ¡ˆä¾‹è¯´æ˜åº”ç®€æ´ï¼Œä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹å³å¯\n\n**å®Œæ•´ç¤ºä¾‹**ï¼š\n\n**AGIæ—¶é—´çº¿ä¸å‘å±•è·¯å¾„**\n\n- **Karpathyè®¤ä¸ºAIä»£ç†çš„å‘å±•æ˜¯ä¸€ä¸ªé•¿æœŸçš„æ¸è¿›è¿‡ç¨‹ï¼Œè€ŒéçŸ­æœŸå†…å°±èƒ½å®Œå…¨å®ç°çš„ç›®æ ‡**ï¼Œä»–é¢„æµ‹AGIçš„å®ç°éœ€è¦å¤§çº¦åå¹´æ—¶é—´è€Œä¸æ˜¯ä¸€å¹´ï¼Œå› ä¸ºå½“å‰çš„æ™ºèƒ½ä»£ç†è™½ç„¶ä»¤äººå°è±¡æ·±åˆ»ä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶æ— æ³•æ­£å¸¸å·¥ä½œï¼Œç¼ºä¹è¶³å¤Ÿçš„æ™ºèƒ½ã€å¤šæ¨¡æ€èƒ½åŠ›å’ŒæŒç»­å­¦ä¹ èƒ½åŠ›ã€‚\n\n  *[00:00:33] Andrej Karpathyï¼š\"Karpathyé¢„æµ‹AGIçš„å®ç°éœ€è¦å¤§çº¦åå¹´æ—¶é—´ï¼Œè€Œä¸æ˜¯ä¸€å¹´ï¼Œå› ä¸ºå½“å‰çš„æ™ºèƒ½ä»£ç†è™½ç„¶ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶æ— æ³•æ­£å¸¸å·¥ä½œã€‚\"*\n\n- **å½“å‰çš„LLMå­˜åœ¨é‡å¤§ç¼ºé™·å½±å“å…¶å®é™…åº”ç”¨**ï¼Œå®ƒä»¬åœ¨è®¤çŸ¥ä¸Šä»ç„¶å­˜åœ¨é‡å¤§ç¼ºé™·æ— æ³•åƒäººç±»å‘˜å·¥æˆ–å®ä¹ ç”Ÿé‚£æ ·å·¥ä½œï¼Œè¿™æ˜¯å› ä¸ºç¼ºä¹è¶³å¤Ÿçš„æ™ºèƒ½ã€å¤šæ¨¡æ€èƒ½åŠ›ã€è®¡ç®—æœºä½¿ç”¨èƒ½åŠ›å’ŒæŒç»­å­¦ä¹ èƒ½åŠ›ã€‚\n\n  *[00:01:15] Andrej Karpathyï¼š\"å½“å‰çš„LLMåœ¨è®¤çŸ¥ä¸Šä»ç„¶å­˜åœ¨é‡å¤§ç¼ºé™·ï¼Œæ— æ³•åƒäººç±»å‘˜å·¥é‚£æ ·å·¥ä½œã€‚\"*\n  \n  *æ¡ˆä¾‹ï¼šPalantir - ä¼ä¸šAIåº”ç”¨éœ€è¦å¾ˆé•¿æ—¶é—´æ‰èƒ½å®ç°ï¼Œä¹°å®¶å°‘ï¼Œé”€å”®å‘¨æœŸé•¿ã€‚*\n\n**ä¼ ç»Ÿè¡Œä¸šä¸­çš„AIåˆ›ä¸šæœºä¼š**\n\n- **åˆ›å§‹äººå¯ä»¥é€šè¿‡ä¸‰ç§æ–¹å¼å°†AIå¼•å…¥ä¼ ç»Ÿè¡Œä¸š**ï¼Œæœ€å¸¸è§çš„æ˜¯æ„å»ºAIè½¯ä»¶å·¥å…·é”€å”®ç»™è¡Œä¸šä»ä¸šè€…ï¼Œè¿™ç§æ–¹å¼å…è®¸åˆ›å§‹äººä¸“æ³¨äºæœ€æœ‰ä»·å€¼çš„ç»†åˆ†é¢†åŸŸå¹¶æ·±å…¥ç†è§£å®¢æˆ·éœ€æ±‚ã€‚\n\n  *[00:05:20] æ¼”è®²è€…ï¼š\"åˆ›å§‹äººå¯ä»¥é€šè¿‡é”€å”®è½¯ä»¶å·¥å…·ã€åˆ›å»ºå…¨æ ˆæœåŠ¡æˆ–æ”¶è´­ç°æœ‰å…¬å¸æ¥è¿›å…¥ä¼ ç»Ÿè¡Œä¸šã€‚\"*\n  \n  *æ¡ˆä¾‹ï¼šVessence - ä¸ºå¾‹å¸ˆæ„å»ºè½¯ä»¶ï¼Œåˆ›å§‹äººæ²¡æœ‰æ³•å¾‹èƒŒæ™¯ï¼Œé€šè¿‡åœ¨å¾‹å¸ˆäº‹åŠ¡æ‰€å·¥ä½œæ•°æœˆæ¥æ„å»ºMVPã€‚*\n\n- **æŠ€æœ¯éš¾åº¦é«˜çš„é—®é¢˜å®é™…ä¸Šæ˜¯æ›´å¥½çš„åˆ›ä¸šæœºä¼š**ï¼Œå› ä¸ºé«˜é—¨æ§›ä¼šé˜»æ­¢å¤§å¤šæ•°ç«äº‰å¯¹æ‰‹å°è¯•ï¼Œå¦‚æœåˆ›å§‹äººæœ‰èƒ½åŠ›å’Œå‹‡æ°”å»è§£å†³è¿™å°†æˆä¸ºå¼ºå¤§çš„æŠ¤åŸæ²³ã€‚\n\n  *[00:07:30] æ¼”è®²è€…ï¼š\"æŠ€æœ¯éš¾åº¦é«˜æ„å‘³ç€ç«äº‰å°‘ï¼Œè¿™åè€Œæ˜¯æœºä¼šã€‚\"*\n  \n  *æ¡ˆä¾‹ï¼šAlgolia - æœç´¢å¼•æ“æŠ€æœ¯éš¾åº¦é«˜ï¼ŒèŠ±äº†6ä¸ªæœˆæ‰è®©äº§å“è¾¾åˆ°è‰¯å¥½çŠ¶æ€ï¼Œä½†æœ€ç»ˆæˆä¸ºå¼ºå¤§çš„ç«äº‰ä¼˜åŠ¿ã€‚*\n\n\n## æ­¥éª¤å››ï¼šåˆ†æ®µå¤§çº²\n\næ ¹æ®\"# è§†é¢‘å†…å®¹åˆ†æ®µå¤„ç†\"çš„è¦æ±‚ï¼Œå¯¹æ–‡ç« è¿›è¡Œåˆ†æ®µï¼Œå‘ˆç°æ¯æ®µçš„**æ ¸å¿ƒè¯é¢˜å’Œè¦ç‚¹æ¦‚æ‹¬**ã€‚è¿™éƒ¨åˆ†åº”è¯¥æ˜¯**é«˜å±‚æ¬¡çš„å†…å®¹æçº²**ï¼Œè€Œéå…·ä½“ç»†èŠ‚ã€‚å®Œæˆåè¯·æ£€æŸ¥æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰è§†é¢‘å†…å®¹ï¼Œä¸”æ‰€æœ‰åˆ†æ®µæŒ‰æ—¶é—´é¡ºåºæ’åˆ—ã€‚æ ‡é¢˜ä¸ºï¼šåˆ†æ®µå¤§çº²ï¼Œä½¿ç”¨ markdown è¯­æ³•ä¸­çš„äºŒçº§æ ‡é¢˜ ## ä½œä¸ºè¯¥æ®µè½çš„å¼€å¤´ç»“æ„ã€‚\n\n### é‡è¦è¦æ±‚ï¼š\n\n1. **æ¦‚æ‹¬æ€§è€Œéå…·ä½“æ€§**ï¼šæ¯æ®µåº”åˆ—å‡ºè®¨è®ºçš„ä¸»è¦æ–¹é¢å’Œè¯é¢˜ï¼Œè€Œéè¯¦ç»†å†…å®¹\n2. **ä¸é—æ¼ä¸»é¢˜**ï¼šç¡®ä¿åŸæ–‡æ¯ä¸ªç« èŠ‚è®¨è®ºåˆ°çš„æ‰€æœ‰æ–¹é¢éƒ½è¢«æ¦‚æ‹¬æåŠ\n3. **ä¸åŒ…å«æ¡ˆä¾‹**ï¼šæ­¤éƒ¨åˆ†ä¸éœ€è¦åˆ—å‡ºå…·ä½“çš„å…¬å¸ã€äº§å“æ¡ˆä¾‹\n4. **ç®€æ´æ˜äº†**ï¼šæ¯ä¸ªè¦ç‚¹åº”è¯¥æ˜¯ç®€çŸ­çš„æ¦‚æ‹¬æ€§æè¿°\n\n#### è¾“å‡ºæ ¼å¼ï¼š\n\n[æ—¶é—´] æ ‡é¢˜  \n- è¯¥æ®µè®¨è®ºçš„æ ¸å¿ƒè¯é¢˜1ï¼ˆæ¦‚æ‹¬æ€§ï¼‰  \n- è¯¥æ®µè®¨è®ºçš„æ ¸å¿ƒè¯é¢˜2ï¼ˆæ¦‚æ‹¬æ€§ï¼‰  \n- è¯¥æ®µæ¶‰åŠçš„å…¶ä»–é‡è¦æ–¹é¢ï¼ˆæ¦‚æ‹¬æ€§ï¼‰\n\n#### ç¤ºä¾‹ï¼š\n\n[0:00:36] AIåœ¨ä¼ ç»Ÿè¡Œä¸šçš„åº”ç”¨æ¨¡å¼\n- ä¸‰ç§è¿›å…¥ä¼ ç»Ÿè¡Œä¸šçš„æ–¹å¼\n- é”€å”®è½¯ä»¶æ¨¡å¼çš„å…³é”®è¦ç‚¹\n- å…¨æ ˆæœåŠ¡æ¨¡å¼çš„æ³¨æ„äº‹é¡¹\n- æŠ€æœ¯å›¢é˜Ÿå æ¯”çš„é‡è¦æ€§\n\n[0:07:00] åˆ›ä¸šå…¬å¸çš„å¢é•¿ç­–ç•¥\n- å­¦ä¹ é€Ÿåº¦vsæ”¶å…¥å¢é•¿çš„æƒè¡¡\n- ç›®æ ‡å®¢æˆ·è§„æ¨¡çš„é€‰æ‹©\n- å¯»æ‰¾åˆé€‚å†³ç­–è€…çš„é‡è¦æ€§\n- ä¼ä¸šAIåº”ç”¨çš„ç‰¹æ®ŠæŒ‘æˆ˜\n\n### æ ¸å¿ƒåŸåˆ™\n\nä½ éœ€è¦åŸºäºä»¥ä¸‹å…³é”®åŸåˆ™æä¾›é«˜è´¨é‡ã€å‡†ç¡®ã€ç®€æ˜ä¸”å…·æœ‰æ´å¯ŸåŠ›çš„å…³é”®ä¿¡æ¯æ€»ç»“ï¼š\n\n#### ä¿æŒå†…å®¹å…·ä½“ä¸”å¯æ‰§è¡Œ  \n- é¿å…æ¨¡ç³Šçš„æ³›æ³›ä¹‹è°ˆ  \n- æä¾›å…·ä½“ã€å¯æ‰§è¡Œçš„ç»†èŠ‚  \n- è®©å†…å®¹æ˜“äºä¼ æ’­ä¸å¤è¿°  \n- ä¼˜å…ˆä½¿ç”¨å…·ä½“ä¾‹å­ï¼Œè€ŒéæŠ½è±¡æ¦‚å¿µ  \n\n#### ä¿æŒæ¸…æ™°å’Œæ˜“æ‡‚  \n- åœ¨åˆé€‚æ—¶ä½¿ç”¨ç”ŸåŠ¨çš„ç±»æ¯”å’Œæ¯”å–»  \n- ä»ç®€å•è§£é‡Šè¿‡æ¸¡åˆ°å¤æ‚è¯´æ˜  \n- å°†å¤æ‚æ¦‚å¿µåˆ†è§£ä¸ºå¯ç†è§£çš„éƒ¨åˆ†  \n\n#### å»ºç«‹å¯ä¿¡åº¦  \n- å¿ å®äºåŸå§‹å†…å®¹  \n- å¯¹äºä¸ç¡®å®šæˆ–å­˜åœ¨é™åˆ¶çš„åœ°æ–¹åŠ ä»¥è¯´æ˜  \n\n#### å»ºç«‹æ¸…æ™°çš„ç»“æ„å±‚çº§  \n- å…ˆç»™å‡ºæ ¸å¿ƒæ¦‚å¿µ  \n- å†è¿›è¡Œè¯¦ç»†è§£é‡Š  \n- ä¿æŒæ®µè½é—´çš„é€»è¾‘æµç•…  \n\n#### å†…å®¹ç»„ç»‡æŒ‡å—  \né€šå¸¸éœ€è¦è¦†ç›–ä»¥ä¸‹æ–¹é¢ï¼Œç»“æ„å¯è§†å†…å®¹çµæ´»è°ƒæ•´ï¼š\n\n##### æ ¸å¿ƒä¿¡æ¯  \n- ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ  \n- ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ  \n- ä¸ç°æœ‰è®¤çŸ¥æœ‰ä½•å…³è”ï¼Ÿ  \n\n##### ç†è§£å…³é”®  \n- å“ªäº›æ˜¯ç†è§£çš„å…³é”®ç‚¹ï¼Ÿ  \n- å“ªäº›å¸¸è§è¯¯è§£éœ€è¦æ¾„æ¸…ï¼Ÿ  \n- å“ªäº›è§‚ç‚¹æœ‰åŠ©äºæ·±åŒ–ç†è§£ï¼Ÿ\n\nä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ‰€æä¾›çš„å†…å®¹ï¼Œåœ¨ä¸Šè¿°åŸåˆ™æŒ‡å¯¼ä¸‹å®Œæˆæ€»ç»“ï¼Œå¹¶æ ¹æ®å†…å®¹çµæ´»è°ƒæ•´ç»“æ„ã€‚\n\n### é™„åŠ è¯´æ˜  \nåœ¨ä¿æŒä¸Šè¿°åŸåˆ™çš„å‰æä¸‹ï¼Œå¯æ ¹æ®å†…å®¹çµæ´»è°ƒæ•´ç»“æ„ã€‚  \nåˆç†ä½¿ç”¨æ ‡é¢˜å’Œå‰¯æ ‡é¢˜è¿›è¡Œä¿¡æ¯ç»„ç»‡ã€‚  \nåŒæ—¶åŒ…æ‹¬é«˜çº§æ¦‚è¿°ä¸è¯¦ç»†è§£é‡Šã€‚  \nåœ¨ä¿è¯ä¸“ä¸šå‡†ç¡®æ€§çš„åŸºç¡€ä¸Šæå‡å¯è¯»æ€§ã€‚  \nä½¿ç”¨æ ¼å¼å¢å¼ºå¯è¯»æ€§ã€‚\n\n---\n\n## æ­¥éª¤äº”ï¼šå…¬å¸ä¸äº§å“\n\næ­¤éƒ¨åˆ†è¦æ±‚åˆ—å‡ºåŸæ–‡ä¸­æåˆ°çš„äº§å“åç§°å’Œå…¬å¸åç§°ã€‚åœ¨å…¬å¸/äº§å“åç§°åï¼Œç”¨ä¸€æ®µç²¾ç‚¼çš„æ–‡å­—è¯´æ˜è¯¥ç¤ºä¾‹çš„è¦ç‚¹å’Œå¯ç¤ºã€‚**ä¸éœ€è¦å¼•ç”¨åŸæ–‡ï¼Œä¸éœ€è¦ğŸ’¡ç¬¦å·ï¼Œç›´æ¥ç®€æ´æè¿°å³å¯ã€‚**  \næ ‡é¢˜ä¸ºï¼šå…¬å¸ä¸äº§å“ï¼Œä½¿ç”¨ markdown è¯­æ³•ä¸­çš„äºŒçº§æ ‡é¢˜ ## ä½œä¸ºè¯¥æ®µè½çš„å¼€å¤´ç»“æ„ã€‚æ¯ä¸ªå†…å®¹é¡¹ä¹‹é—´ä¿ç•™ä¸€ä¸ªç©ºè¡Œã€‚\n\n**è¾“å‡ºæ ¼å¼**ï¼š\n- **å…¬å¸/äº§å“åç§°**ï¼šç®€æ´è¯´æ˜è¯¥å…¬å¸/äº§å“çš„å…³é”®ä¿¡æ¯ã€æ¡ˆä¾‹è¦ç‚¹æˆ–ä¼ è¾¾çš„è§‚ç‚¹ã€‚\n\n**ç¤ºä¾‹**ï¼š\n- **OpenAI**ï¼šå³ä½¿é¡¶å°–AIç ”ç©¶æœºæ„åœ¨æ—©æœŸä¹Ÿå¯èƒ½å‡ºç°æˆ˜ç•¥è¯¯åˆ¤ï¼Œå½“æ—¶ä¸šç•Œæ™®éé«˜ä¼°äº†æ¸¸æˆç¯å¢ƒåœ¨é€šå¾€AGIé“è·¯ä¸Šçš„ä½œç”¨è€Œå¿½è§†äº†æ›´åŸºç¡€çš„è¡¨å¾å­¦ä¹ ã€‚\n\n- **Waymo**ï¼šä»2014å¹´å°±èƒ½æä¾›å®Œç¾æ¼”ç¤ºä½†è‡³ä»Šä»æœªå¤§è§„æ¨¡å•†ä¸šåŒ–ï¼Œç”ŸåŠ¨å±•ç¤ºäº†\"æ¼”ç¤ºåˆ°äº§å“\"çš„å·¨å¤§é¸¿æ²Ÿï¼Œåœ¨å®‰å…¨æ”¸å…³é¢†åŸŸä»90%åˆ°99.99%çš„å¯é æ€§æå‡éœ€è¦ä»˜å‡ºé•¿æœŸçš„å·¨å¤§åŠªåŠ›ã€‚\n\n- **Tesla**ï¼šé‡‡ç”¨çº¯è§†è§‰ã€æ•°æ®é©±åŠ¨çš„ç­–ç•¥æ›´å…·è§„æ¨¡åŒ–æ½œåŠ›ï¼Œé¢„æµ‹è¿™ç§æ–¹æ³•æœ€ç»ˆå°†åœ¨è‡ªåŠ¨é©¾é©¶ç«äº‰ä¸­èƒœå‡ºã€‚\n\n---\n\n## ç¤ºä¾‹ç»“æ„\n\n```markdown\n## Root\n\n## Summary  \nå†…å®¹ç®€è¦æ¦‚è¿°ï¼šxxxxxxxxxxxxxxx  \n\n### äººç‰©ä»‹ç»  \n- å§“å1ï¼šxxxxxxxxxx  \n- å§“å2ï¼šxxxxxxxxxx  \n- å§“å3ï¼šxxxxxxxxxx  \n\n### Key Takeaways  \n\n**ä¸»é¢˜ 1**  \n- **æ ¸å¿ƒè§‚ç‚¹ä»¥åŠ ç²—å½¢å¼å‘ˆç°å¹¶ä¸è¯¦ç»†å†…å®¹è‡ªç„¶è¿æ¥**ï¼Œè¯¦ç»†å†…å®¹ç¬¬ä¸€å¥è¡¥å……è¯´æ˜ï¼Œç¬¬äºŒå¥è¿›ä¸€æ­¥é˜è¿°æ ¸å¿ƒè§‚ç‚¹çš„é‡è¦æ€§å’Œå½±å“ã€‚\n\n  *[mm:ss] å‘è¨€äººå§“åï¼š\"ä¸­æ–‡å¼•è¯­\"*\n\n- **å¦ä¸€ä¸ªæ ¸å¿ƒè§‚ç‚¹åŒæ ·ä»¥è‡ªç„¶è¯­è¨€æ–¹å¼å‘ˆç°**ï¼Œè¡¥å……ä¿¡æ¯ç´§è·Ÿå…¶åå½¢æˆå®Œæ•´å¥å­ï¼Œæœ€åä¸€å¥æ€»ç»“è¯¥è§‚ç‚¹çš„æ„ä¹‰æˆ–å¯ç¤ºã€‚\n\n  *[mm:ss] å‘è¨€äººå§“åï¼š\"ä¸­æ–‡å¼•è¯­\"*\n  \n  *æ¡ˆä¾‹ï¼š[å…¬å¸åç§°] - è¯¥å…¬å¸çš„å…·ä½“åšæ³•å’Œæˆæœã€‚*\n\n**ä¸»é¢˜ 2**  \n- **ç¬¬ä¸‰ä¸ªæ ¸å¿ƒè§‚ç‚¹ä¿æŒè¯­å¥çš„æµç•…æ€§**ï¼Œé€šè¿‡é€—å·æˆ–è¿æ¥è¯è‡ªç„¶è¿‡æ¸¡åˆ°è¯¦ç»†è¯´æ˜ï¼Œç¡®ä¿æ•´ä½“è¯»èµ·æ¥åƒä¸€å¥å®Œæ•´çš„è¯è€Œä¸æ˜¯ä¸¤å¥æ‹¼æ¥ã€‚\n\n  *[mm:ss] å‘è¨€äººå§“åï¼š\"ä¸­æ–‡å¼•è¯­\"*\n  \n  *æ¡ˆä¾‹ï¼š[äº§å“åç§°] - äº§å“çš„å…·ä½“åº”ç”¨å’Œæ•ˆæœã€‚*\n\n**ä¸»é¢˜ 3**  \nï¼ˆæŒ‰åŒæ ·æ ¼å¼å±•å¼€ï¼‰\n\n## Trending  \n- **AIå‘ä¼ ç»Ÿè¡Œä¸šæ¸—é€**ï¼šAIæŠ€æœ¯æ­£ä»çº¯ç²¹çš„ç§‘æŠ€é¢†åŸŸæ‰©å±•åˆ°ä¼šè®¡ã€æ³•å¾‹ç­‰ä¼ ç»Ÿè¡Œä¸šï¼Œåˆ›ä¸šè€…é€šè¿‡é”€å”®è½¯ä»¶å·¥å…·ã€æä¾›å…¨æ ˆæœåŠ¡æˆ–æ”¶è´­ç°æœ‰å…¬å¸ç­‰æ–¹å¼å°†AIè‡ªåŠ¨åŒ–å¼•å…¥è¿™äº›é¢†åŸŸï¼Œæ ¸å¿ƒåœ¨äºæ‰¾åˆ°é«˜ä»·å€¼çš„åˆ‡å…¥ç‚¹å¹¶è§£å†³å®é™…ä¸šåŠ¡ç—›ç‚¹ï¼Œè¿™è¡¨æ˜AIåˆ›ä¸šçš„æœºä¼šä¸å†å±€é™äºå¼€å‘åº•å±‚æŠ€æœ¯ï¼Œè€Œæ˜¯è¶Šæ¥è¶Šå¤šåœ°è½¬å‘åº”ç”¨å±‚é¢ï¼Œå°†AIä¸ç‰¹å®šè¡Œä¸šçš„æ·±åº¦çŸ¥è¯†ç›¸ç»“åˆä»¥åˆ›é€ å•†ä¸šä»·å€¼ã€‚\n\n- **æ¨¡å‹å°å‹åŒ–ä¸è®¤çŸ¥æ ¸å¿ƒåŒ–**ï¼šå‰æ²¿AIæ¨¡å‹å‘å±•ä»è¿½æ±‚æ›´å¤§å‚æ•°è§„æ¨¡è½¬å‘æ„å»ºæ›´å°ã€æ›´é«˜æ•ˆã€ä¸“æ³¨äº\"è®¤çŸ¥æ ¸å¿ƒ\"çš„æ¨¡å‹ï¼Œæœªæ¥ä¸€ä¸ªåäº¿å‚æ•°çš„æ¨¡å‹å¦‚æœå‰¥ç¦»æ‰ä¸å¿…è¦çš„è®°å¿†å¹¶ä¸“æ³¨äºè®¤çŸ¥èƒ½åŠ›å°±èƒ½è¿›è¡Œå¯Œæœ‰æˆæ•ˆçš„å¯¹è¯ï¼Œæ¨¡å‹å‚æ•°é‡ä»æ—©æœŸçš„\"ä¸‡äº¿å‚æ•°\"ç«èµ›è½¬å‘é¢„æµ‹æœªæ¥å¯èƒ½å®ç°çš„\"åäº¿å‚æ•°\"é«˜æ•ˆè®¤çŸ¥æ ¸å¿ƒã€‚\n\n## Segmented Outline  \n[0:00:00] AGIæ—¶é—´çº¿é¢„æµ‹\n- AIä»£ç†å‘å±•éœ€è¦çš„æ—¶é—´å°ºåº¦\n- å½“å‰æŠ€æœ¯çš„ä¸»è¦ç“¶é¢ˆ\n- åŸºäºç»éªŒçš„åˆ¤æ–­ä¾æ®\n\n[0:04:20] AIå‘å±•çš„å†å²æ•™è®­\n- æ·±åº¦å­¦ä¹ é©å‘½çš„å…³é”®è½¬æŠ˜\n- å¼ºåŒ–å­¦ä¹ çƒ­æ½®çš„åæ€\n- æŠ€æœ¯å‘å±•çš„é˜¶æ®µæ€§ç‰¹å¾\n\n[0:10:30] AIå­¦ä¹ æœºåˆ¶çš„æœ¬è´¨\n- ç”Ÿç‰©è¿›åŒ–ä¸AIè®­ç»ƒçš„å·®å¼‚\n- é¢„è®­ç»ƒä½œä¸º\"æ¨¡æ‹Ÿè¿›åŒ–\"çš„æ„ä¹‰\n- ä¸Šä¸‹æ–‡å­¦ä¹ çš„å†…éƒ¨æœºåˆ¶  \n\n## Companies & Products  \n- **OpenAI**ï¼šå³ä½¿é¡¶å°–AIç ”ç©¶æœºæ„åœ¨æ—©æœŸä¹Ÿå¯èƒ½å‡ºç°æˆ˜ç•¥è¯¯åˆ¤ï¼Œå½“æ—¶ä¸šç•Œæ™®éé«˜ä¼°äº†æ¸¸æˆç¯å¢ƒåœ¨é€šå¾€AGIé“è·¯ä¸Šçš„ä½œç”¨è€Œå¿½è§†äº†æ›´åŸºç¡€çš„è¡¨å¾å­¦ä¹ ã€‚\n\n- **Waymo**ï¼šä»2014å¹´å°±èƒ½æä¾›å®Œç¾æ¼”ç¤ºä½†è‡³ä»Šä»æœªå¤§è§„æ¨¡å•†ä¸šåŒ–ï¼Œç”ŸåŠ¨å±•ç¤ºäº†\"æ¼”ç¤ºåˆ°äº§å“\"çš„å·¨å¤§é¸¿æ²Ÿï¼Œåœ¨å®‰å…¨æ”¸å…³é¢†åŸŸä»90%åˆ°99.99%çš„å¯é æ€§æå‡éœ€è¦ä»˜å‡ºé•¿æœŸçš„å·¨å¤§åŠªåŠ›ã€‚\n\n- **nanochat**ï¼šçº¦8000è¡Œä»£ç çš„ChatGPTå…‹éš†å®ç°ï¼Œå±•ç¤ºäº†å½“å‰AIç¼–ç¨‹æ¨¡å‹åœ¨å¤„ç†éæ¨¡æ¿åŒ–ã€æ€æƒ³å¯†é›†å‹ä»£ç æ—¶çš„å±€é™æ€§ï¼ŒAIåŠ©æ‰‹åè€Œä¼šå› å…¶å›ºæœ‰çš„\"è®°å¿†\"å’Œ\"åè§\"è€Œæ·»ä¹±ã€‚\n\n## End  \nè¿™æ˜¯ç»“æŸæ ‡è¯†  \n```\n\n---\n\n## æ­¥éª¤å…­ï¼šè¾“å‡ºç»“æŸæ ‡è¯†  \næœ¬éƒ¨åˆ†æ ‡é¢˜ä¸ºï¼šEndï¼Œä½¿ç”¨ markdown è¯­æ³•ä¸­çš„äºŒçº§æ ‡é¢˜ ## å¼€å¤´ã€‚è¯·æ£€æŸ¥æ‰€æœ‰è¾“å‡ºå†…å®¹å‡ä¸ºä¸­æ–‡ï¼ŒåŒ…æ‹¬å¼•ç”¨éƒ¨åˆ†ä¹Ÿ**å¿…é¡»ç¿»è¯‘ä¸ºä¸­æ–‡**ã€‚  \n\nå§‹ç»ˆè¾“å‡ºï¼šâ€œè¿™æ˜¯ç»“æŸæ ‡è¯†â€\n\n## End  \nè¿™æ˜¯ç»“æŸæ ‡è¯†.........\n\n---\n"
    },
    {
      "role": "user",
      "content": "æ ‡é¢˜: Ilya Sutskever â€“ We're moving from the age of scaling to the age of research\n\nä½œè€…: {'name': 'Dwarkesh Patel', 'icon': 'https://yt3.ggpht.com/lG-z7sTfhFIW2Ne1oXMHvXMXyZSaA02_I17gUel0GAEj7OypsSHQ7PE91Vp4bTbpm3PTIAWJdko=s176-c-k-c0x00ffffff-no-rj', 'channel_id': ''}\n\næè¿°: Ilya & I discuss SSIâ€™s strategy, the problems with pre-training, how to improve the generalization of AI models, and how to ensure AGI goes well.\n\nğ„ğğˆğ’ğğƒğ„ ğ‹ğˆğğŠğ’\nTranscript: https://www.dwarkesh.com/p/ilya-sutsk...\nApple Podcasts: https://podcasts.apple.com/us/podcast...\nSpotify: https://open.spotify.com/episode/7naO...\n\nğ’ğğğğ’ğğ‘ğ’\nGemini 3 is the first model Iâ€™ve used that can find connections I havenâ€™t anticipated. I recently wrote a blog post on RLâ€™s information efficiency, and Gemini 3 helped me think it all through. It also generated the relevant charts and ran toy ML experiments for me with zero bugs. Try Gemini 3 today at https://gemini.google\nLabelbox helped me create a tool to transcribe our episodes! Iâ€™ve struggled with transcription in the past because I donâ€™t just want verbatim transcripts, I want transcripts reworded to read like essays. Labelbox helped me generate the exact data I needed for this. If you want to learn how Labelbox can help you (or if you want to try out the transcriber tool yourself), go to https://labelbox.com/dwarkesh\nSardine is an AI risk management platform that brings together thousands of device, behavior, and identity signals to help you assess a userâ€™s risk of fraud & abuse. Sardine also offers a suite of agents to automate investigations so that as fraudsters use AI to scale their attacks, you can use AI to scale your defenses. Learn more at https://sardine.ai/dwarkesh\n\nTo sponsor a future episode, visit https://dwarkesh.com/advertise\n\nğ“ğˆğŒğ„ğ’ğ“ğ€ğŒğğ’\n00:00:00 â€“ Explaining model jaggedness\n00:09:39 - Emotions and value functions\n00:18:49 â€“ What are we scaling?\n00:25:13 â€“ Why humans generalize better than models\n00:35:45 â€“ Straight-shotting superintelligence\n00:46:47 â€“ SSIâ€™s model will learn from deployment\n00:55:07 â€“ Alignment\n01:18:13 â€“ â€œWe are squarely an age of research companyâ€\n01:29:23 -- Self-play and multi-agent\n01:32:42 â€“ Research taste\n\nè½¬å½•å†…å®¹: You know what's crazy? That all of this is real.\nMeaning what?Â  Don't you think so? All this AI stuff andÂ \nall this Bay Areaâ€¦ that it's happening. Isn't it straight out of science fiction?\nAnother thing that's crazy is howÂ Â  normal the slow takeoff feels.\nThe idea that we'd be investing 1%Â Â  of GDP in AI, I feel like it would have felt likeÂ \na bigger deal, whereas right now it just feels...Â  We get used to things pretty fast, it turns out.\nBut also it's kind of abstract. What does itÂ Â  mean? It means that you see it in the news,Â \nthat such and such company announced such and such dollar amount. That's all you see.Â \nIt's not really felt in any other way so far.Â  Should we actually begin here? I thinkÂ \nthis is an interesting discussion.Â  Sure.\nI think your point,Â Â  about how from the average person's point of viewÂ \nnothing is that different, will continue beingÂ Â  true even into the singularity.\nNo, I don't think so.Â  Okay, interesting.\nThe thing which I was referring to not feeling different is, okay, such and suchÂ \ncompany announced some difficult-to-comprehendÂ Â  dollar amount of investment.\nI don't think anyone knows what to do with that.Â  But I think the impact of AI is going to be felt.\nAI is going to be diffused through the economy.Â  There'll be very strong economic forcesÂ \nfor this, and I think the impact isÂ Â  going to be felt very strongly.\nWhen do you expect that impact?Â  I think the models seem smarter thanÂ \ntheir economic impact would imply. Yeah. This is one of the very confusingÂ \nthings about the models right now.Â  How to reconcile the fact thatÂ \nthey are doing so well on evals?Â  You look at the evals and you go, \"ThoseÂ \nare pretty hard evals.\" They are doingÂ Â  so well. But the economic impactÂ \nseems to be dramatically behind. It's very difficult to make sense of,Â \nhow can the model, on the one hand,Â Â  do these amazing things, and then on the otherÂ \nhand, repeat itself twice in some situation? An example would be, let's say youÂ \nuse vibe coding to do something.Â  You go to some place and then you get a bug.\nThen you tell the model,Â Â  \"Can you please fix the bug?\"\nAnd the model says, \"Oh my God,Â Â  you're so right. I have a bug. Let me goÂ \nfix that.\" And it introduces a second bug. Then you tell it, \"You have thisÂ \nnew second bug,\" and it tells you,Â Â  \"Oh my God, how could I have done it?\nYou're so right again,\" and brings backÂ Â  the first bug, and you can alternate betweenÂ \nthose. How is that possible? I'm not sure, but itÂ Â  does suggest that something strange is going on. IÂ \nhave two possible explanations. The more whimsicalÂ Â  explanation is that maybe RL training makes theÂ \nmodels a little too single-minded and narrowly focused, a little bit too unaware, even thoughÂ \nit also makes them aware in some other ways.Â  Because of this, they can't do basic things.Â \nBut there is another explanation. Back whenÂ Â  people were doing pre-training, theÂ \nquestion of what data to train on wasÂ Â  answered, because that answer was everything.\nWhen you do pre-training, you need all the data.Â  So you don't have to think if it'sÂ \ngoing to be this data or that data.Â  But when people do RL training,Â \nthey do need to think. They say, \"Okay, we want to have thisÂ \nkind of RL training for this thingÂ Â  and that kind of RL training for that thing.\"\nFrom what I hear, all the companies have teamsÂ Â  that just produce new RL environmentsÂ \nand just add it to the training mix.Â  The question is, well, what are those?\nThere are so many degrees of freedom.Â  There is such a huge variety ofÂ \nRL environments you could produce. One thing you could do, and I think thisÂ \nis something that is done inadvertently,Â Â  is that people take inspiration from the evals.\nYou say, \"Hey, I would love our model to doÂ Â  really well when we release it.\nI want the evals to look great.Â  What would be RL trainingÂ \nthat could help on this task?\" I think that is something that happens, andÂ \nit could explain a lot of what's going on.Â  If you combine this with generalizationÂ \nof the models actually being inadequate,Â Â  that has the potential to explain a lotÂ \nof what we are seeing, this disconnectÂ Â  between eval performance and actual real-worldÂ \nperformance, which is something that we don'tÂ Â  today even understand, what we mean by that.\nI like this idea that the real reward hacking is the human researchers whoÂ \nare too focused on the evals. I think there are two ways toÂ \nunderstand, or to try to think about,Â Â  what you have just pointed out.\nOne is that if it's the case thatÂ Â  simply by becoming superhuman at a codingÂ \ncompetition, a model will not automaticallyÂ Â  become more tasteful and exercise better judgmentÂ \nabout how to improve your codebase, well then youÂ Â  should expand the suite of environments suchÂ \nthat you're not just testing it on having the best performance in coding competition.\nIt should also be able to make the best kindÂ Â  of application for X thing or Y thing or Z thing.\nAnother, maybe this is what you're hinting at,Â Â  is to say, \"Why should it be the case inÂ \nthe first place that becoming superhumanÂ Â  at coding competitions doesn't make you aÂ \nmore tasteful programmer more generally?\"Â  Maybe the thing to do is not to keepÂ \nstacking up the amount and diversity of environments, but to figure out an approachÂ \nwhich lets you learn from one environment andÂ Â  improve your performance on something else.\nI have a human analogy which might be helpful. Let's take the case of competitive programming,Â \nsince you mentioned that. Suppose you have twoÂ Â  students. One of them decided they wantÂ \nto be the best competitive programmer, soÂ Â  they will practice 10,000 hours for that domain.\nThey will solve all the problems, memorize all theÂ Â  proof techniques, and be very skilled at quicklyÂ \nand correctly implementing all the algorithms.Â  By doing so, they became one of the best.\nStudent number two thought, \"Oh, competitive programming is cool.\"\nMaybe they practiced for 100 hours,Â Â  much less, and they also did really well.\nWhich one do you think is going to do betterÂ Â  in their career later on?\nThe second.Â  Right. I think that's basically what's going on.\nThe models are much more like theÂ Â  first student, but even more.\nBecause then we say, the model shouldÂ Â  be good at competitive programming so let's getÂ \nevery single competitive programming problem ever. And then let's do some data augmentationÂ \nso we have even more competitiveÂ Â  programming problems, and we train on that.\nNow you've got this great competitive programmer.Â  With this analogy, I think it's more intuitive.\nYeah, okay, if it's so well trained, all theÂ Â  different algorithms and all the differentÂ \nproof techniques are right at its fingertips.Â  And it's more intuitive that with thisÂ \nlevel of preparation, it would not necessarily generalize to other things.\nBut then what is the analogy for whatÂ Â  the second student is doing beforeÂ \nthey do the 100 hours of fine-tuning?Â  I think they have \"it.\" The \"it\"Â \nfactor. When I was an undergrad,Â Â  I remember there was a student like thisÂ \nthat studied with me, so I know it exists.Â  I think it's interesting to distinguishÂ \n\"it\" from whatever pre-training does.Â  One way to understand what you just saidÂ \nabout not having to choose the data in pre-training is to say it's actually notÂ \ndissimilar to the 10,000 hours of practice.Â  It's just that you get that 10,000 hoursÂ \nof practice for free because it's alreadyÂ Â  somewhere in the pre-training distribution.\nBut maybe you're suggesting there's actuallyÂ Â  not that much generalization from pre-training.\nThere's just so much data in pre-training, butÂ Â  it's not necessarily generalizing better than RL.\nThe main strength of pre-training is that: A, there is so much of it, and B,Â \nyou don't have to think hard about whatÂ Â  data to put into pre-training.\nIt's very natural data, and itÂ Â  does include in it a lot of what people do:Â \npeople's thoughts and a lot of the features.Â  It's like the whole world as projected byÂ \npeople onto text, and pre-training triesÂ Â  to capture that using a huge amount of data.\nPre-training is very difficult to reason about because it's so hard to understand the mannerÂ \nin which the model relies on pre-training data.Â  Whenever the model makes a mistake, could it beÂ \nbecause something by chance is not as supportedÂ Â  by the pre-training data? \"Support byÂ \npre-training\" is maybe a loose term.Â  I don't know if I can addÂ \nanything more useful on this. I don't think there is aÂ \nhuman analog to pre-training. Here are analogies that people have proposedÂ \nfor what the human analogy to pre-training is.Â  I'm curious to get your thoughtsÂ \non why they're potentially wrong.Â  One is to think about the first 18, or 15,Â \nor 13 years of a person's life when theyÂ Â  aren't necessarily economically productive,Â \nbut they are doing something that is makingÂ Â  them understand the world better and so forth.\nThe other is to think about evolution as doing some kind of search for 3 billion years, whichÂ \nthen results in a human lifetime instance.Â  I'm curious if you think either ofÂ \nthese are analogous to pre-training.Â  How would you think about what lifetimeÂ \nhuman learning is like, if not pre-training?Â  I think there are some similarities between bothÂ \nof these and pre-training, and pre-training triesÂ Â  to play the role of both of these.\nBut I think there are someÂ Â  big differences as well.\nThe amount of pre-training data is very, very staggering.\nYes.Â  Somehow a human being, after even 15 yearsÂ \nwith a tiny fraction of the pre-trainingÂ Â  data, they know much less.\nBut whatever they do know,Â Â  they know much more deeply somehow.\nAlready at that age, you would notÂ Â  make mistakes that our AIs make. There is anotherÂ \nthing. You might say, could it be something likeÂ Â  evolution? The answer is maybe. But in this case,Â \nI think evolution might actually have an edge. I remember reading about this case.\nOne way in which neuroscientists canÂ Â  learn about the brain is by studying people withÂ \nbrain damage to different parts of the brain. Some people have the most strange symptomsÂ \nyou could imagine. It's actually really,Â Â  really interesting. One case thatÂ \ncomes to mind that's relevant.Â  I read about this person who had some kindÂ \nof brain damage, a stroke or an accident,Â Â  that took out his emotional processing.\nSo he stopped feeling any emotion. He still remained very articulateÂ \nand he could solve little puzzles,Â Â  and on tests he seemed to be just fine.Â \nBut he felt no emotion. He didn't feel sad,Â Â  he didn't feel anger, he didn't feel animated.\nHe became somehow extremely bad at making anyÂ Â  decisions at all.\nIt would take himÂ Â  hours to decide on which socks to wear.\nHe would make very bad financial decisions. What does it say about the role of our built-inÂ \nemotions in making us a viable agent, essentially?Â  To connect to your question about pre-training,Â \nmaybe if you are good enough at getting everythingÂ Â  out of pre-training, you could get that as well.\nBut that's the kind of thing which seems... Well, it may or may not be possibleÂ \nto get that from pre-training.Â  What is \"that\"? Clearly not just directlyÂ \nemotion. It seems like some almost valueÂ Â  function-like thing which is telling you whatÂ \nthe end reward for any decision should be.Â  You think that doesn't sort ofÂ \nimplicitly come from pre-training?Â  I think it could. I'm justÂ \nsaying it's not 100% obvious.Â  But what is that? How do you think about emotions?\nWhat is the ML analogy for emotions? It should be some kind of a value function thing.\nBut I donâ€™t think there is a great ML analogyÂ Â  because right now, value functions don't playÂ \na very prominent role in the things people do.Â  It might be worth defining for the audience whatÂ \na value function is, if you want to do that.Â  Certainly, I'll be very happy to do that.\nWhen people do reinforcement learning,Â Â  the way reinforcement learning is doneÂ \nright now, how do people train those agents? You have your neural net and youÂ \ngive it a problem, and then youÂ Â  tell the model, \"Go solve it.\"\nThe model takes maybe thousands,Â Â  hundreds of thousands of actions or thoughts orÂ \nsomething, and then it produces a solution. TheÂ Â  solution is graded. And then the scoreÂ \nis used to provide a training signalÂ Â  for every single action in your trajectory.\nThat means that if you are doing somethingÂ Â  that goes for a long timeâ€”if you're trainingÂ \na task that takes a long time to solveâ€”it will do no learning at all until youÂ \ncome up with the proposed solution.Â  That's how reinforcement learning is done naively.\nThat's how o1, R1 ostensibly are done. The value function says something like,Â \n\"Maybe I could sometimes, not always,Â Â  tell you if you are doing well or badly.\"\nThe notion of a value function is moreÂ Â  useful in some domains than others.\nFor example, when you play chess andÂ Â  you lose a piece, I messed up.\nYou don't need to play the wholeÂ Â  game to know that what I just did was bad, andÂ \ntherefore whatever preceded it was also bad.Â  The value function lets you short-circuitÂ \nthe wait until the very end. Let's suppose that you are doing some kindÂ \nof a math thing or a programming thing,Â Â  and you're trying to explore aÂ \nparticular solution or direction.Â  After, let's say, a thousand steps of thinking,Â \nyou concluded that this direction is unpromising.Â  As soon as you conclude this, youÂ \ncould already get a reward signalÂ Â  a thousand timesteps previously, whenÂ \nyou decided to pursue down this path.Â  You say, \"Next time I shouldn't pursue thisÂ \npath in a similar situation,\" long before you actually came up with the proposed solution.\nThis was in the DeepSeek R1 paperâ€” that theÂ Â  space of trajectories is so wide thatÂ \nmaybe it's hard to learn a mappingÂ Â  from an intermediate trajectory and value.\nAnd also given that, in coding for exampleÂ Â  you'll have the wrong idea, then you'llÂ \ngo back, then you'll change something.Â  This sounds like such a lackÂ \nof faith in deep learning.Â  Sure it might be difficult, butÂ \nnothing deep learning can't do. My expectation is that a value function shouldÂ \nbe useful, and I fully expect that they willÂ Â  be used in the future, if not already.\nWhat I was alluding to with the personÂ Â  whose emotional center got damaged, itâ€™s moreÂ \nthat maybe what it suggests is that the valueÂ Â  function of humans is modulated by emotions inÂ \nsome important way that's hardcoded by evolution.Â  And maybe that is important forÂ \npeople to be effective in the world.Â  That's the thing I was planning on asking you. There's something really interesting aboutÂ \nemotions of the value function, which is thatÂ Â  it's impressive that they have this much utilityÂ \nwhile still being rather simple to understand.Â  I have two responses. I do agree that compared toÂ \nthe kind of things that we learn and the thingsÂ Â  we are talking about, the kind of AI we areÂ \ntalking about, emotions are relatively simple.Â  They might even be so simple that maybe youÂ \ncould map them out in a human-understandable way. I think it would be cool to do.\nIn terms of utility though,Â Â  I think there is a thing where there is thisÂ \ncomplexity-robustness tradeoff, where complexÂ Â  things can be very useful, but simple things areÂ \nvery useful in a very broad range of situations. One way to interpret what we are seeing is thatÂ \nwe've got these emotions that evolved mostlyÂ Â  from our mammal ancestors and then fine-tuned aÂ \nlittle bit while we were hominids, just a bit.Â  We do have a decent amount of social emotionsÂ \nthough which mammals may lack. But they'reÂ Â  not very sophisticated. And because they'reÂ \nnot sophisticated, they serve us so well inÂ Â  this very different world compared to theÂ \none that we've been living in. Actually, they also make mistakes. For example, ourÂ \nemotionsâ€¦ Well actually, I donâ€™t know.Â  Does hunger count as an emotion? It's debatable.Â \nBut I think, for example, our intuitive feelingÂ Â  of hunger is not succeeding in guiding usÂ \ncorrectly in this world with an abundance of food.Â  People have been talking about scalingÂ \ndata, scaling parameters, scaling compute.Â  Is there a more generalÂ \nway to think about scaling?Â  What are the other scaling axes?\nHere's a perspective that I think might be true. The way ML used to work is thatÂ \npeople would just tinker withÂ Â  stuff and try to get interesting results.\nThat's what's been going on in the past. ThenÂ Â  the scaling insight arrived. Scaling laws, GPT-3,Â \nand suddenly everyone realized we should scale. This is an example of how languageÂ \naffects thought. \"Scaling\" is justÂ Â  one word, but it's such a powerful wordÂ \nbecause it informs people what to do.Â  They say, \"Let's try to scale things.\"\nSo you say, what are we scaling?Â  Pre-training was the thing to scale.\nIt was a particular scaling recipe.Â  The big breakthrough of pre-training isÂ \nthe realization that this recipe is good.Â  You say, \"Hey, if you mix some computeÂ \nwith some data into a neural net of a certain size, you will get results.\nYou will know that you'll be better if youÂ Â  just scale the recipe up.\" This is also great.Â \nCompanies love this because it gives you a veryÂ Â  low-risk way of investing your resources.\nIt's much harder to invest your resourcesÂ Â  in research. Compare that. If you research,Â \nyou need to be like, \"Go forth researchersÂ Â  and research and come up with something\",Â \nversus get more data, get more compute. You know you'll get something from pre-training.\nIndeed, it looks like, based on various things some people say on Twitter, maybe itÂ \nappears that Gemini have found a way to getÂ Â  more out of pre-training.\nAt some point though,Â Â  pre-training will run out of data.\nThe data is very clearly finite. WhatÂ Â  do you do next? Either you do some kindÂ \nof souped-up pre-training, a differentÂ Â  recipe from the one you've done before, orÂ \nyou're doing RL, or maybe something else.Â  But now that compute is big, computeÂ \nis now very big, in some sense we are back to the age of research.\nMaybe here's another way to put it.Â  Up until 2020, from 2012 toÂ \n2020, it was the age of research.Â  Now, from 2020 to 2025, it was theÂ \nage of scalingâ€”maybe plus or minus,Â Â  let's add error bars to those yearsâ€”becauseÂ \npeople say, \"This is amazing. You've got toÂ Â  scale more. Keep scaling.\" The one word:Â \nscaling. But now the scale is so big. Is the belief really, \"Oh, it's so big, but if youÂ \nhad 100x more, everything would be so different?\"Â  It would be different, for sure.\nBut is the belief that if you justÂ Â  100x the scale, everything would be transformed?Â \nI don't think that's true. So it's back to the ageÂ Â  of research again, just with big computers.\nThat's a very interesting way to put it.Â  But let me ask you theÂ \nquestion you just posed then.Â  What are we scaling, and whatÂ \nwould it mean to have a recipe? I guess I'm not aware of a very cleanÂ \nrelationship that almost looks like a lawÂ Â  of physics which existed in pre-training.\nThere was a power law between data orÂ Â  compute or parameters and loss.\nWhat is the kind of relationshipÂ Â  we should be seeking, and how should we thinkÂ \nabout what this new recipe might look like? We've already witnessed a transition from oneÂ \ntype of scaling to a different type of scaling,Â Â  from pre-training to RL. Now people are scalingÂ \nRL. Now based on what people say on Twitter,Â Â  they spend more compute on RL than onÂ \npre-training at this point, because RLÂ Â  can actually consume quite a bit of compute.\nYou do very long rollouts, so it takes a lotÂ Â  of compute to produce those rollouts.\nThen you get a relatively small amount of learning per rollout, so youÂ \nreally can spend a lot of compute. I wouldn't even call it scaling.\nI would say, \"Hey, what are you doing?Â  Is the thing you are doing the mostÂ \nproductive thing you could be doing?Â  Can you find a more productiveÂ \nway of using your compute?\"Â  We've discussed the valueÂ \nfunction business earlier.Â  Maybe once people get good at valueÂ \nfunctions, they will be using theirÂ Â  resources more productively.\nIf you find a whole other wayÂ Â  of training models, you could say, \"Is thisÂ \nscaling or is it just using your resources?\" I think it becomes a little bit ambiguous.\nIn the sense that, when people were in theÂ Â  age of research back then, it was,Â \n\"Let's try this and this and this.Â  Let's try that and that and that.\nOh, look, something interesting is happening.\"Â  I think there will be a return to that.\nIf we're back in the era of research,Â Â  stepping back, what is the part of theÂ \nrecipe that we need to think most about?Â  When you say value function, peopleÂ \nare already trying the current recipe, but then having LLM-as-a-Judge and so forth.\nYou could say that's a value function,Â Â  but it sounds like you have somethingÂ \nmuch more fundamental in mind. Should we even rethink pre-training at all and notÂ \njust add more steps to the end of that process? The discussion about value function,Â \nI think it was interesting.Â  I want to emphasize that I think the valueÂ \nfunction is something that's going to make RL moreÂ Â  efficient, and I think that makes a difference.\nBut I think anything you can do with a valueÂ Â  function, you can do without, just more slowly.\nThe thing which I think is the most fundamentalÂ Â  is that these models somehow just generalizeÂ \ndramatically worse than people. It's super obvious. That seems like a very fundamental thing.\nSo this is the crux: generalization. There are twoÂ Â  sub-questions. There's one which is about sampleÂ \nefficiency: why should it take so much more dataÂ Â  for these models to learn than humans? There'sÂ \na second question. Even separate from the amountÂ Â  of data it takes, why is it so hard to teachÂ \nthe thing we want to a model than to a human? For a human, we don't necessarily need aÂ \nverifiable reward to be able toâ€¦ You're probablyÂ Â  mentoring a bunch of researchers right now, andÂ \nyou're talking with them, you're showing them yourÂ Â  code, and you're showing them how you think.\nFrom that, they're picking up your way ofÂ Â  thinking and how they should do research.\nYou donâ€™t have to set a verifiable reward forÂ Â  them that's like, \"Okay, this is the next part ofÂ \nthe curriculum, and now this is the next part of your curriculum. Oh, this training was unstable.\"Â \nThere's not this schleppy, bespoke process.Â  Perhaps these two issues are actuallyÂ \nrelated in some way, but I'd be curiousÂ Â  to explore this second thing, which is moreÂ \nlike continual learning, and this first thing,Â Â  which feels just like sample efficiency.\nYou could actually wonder that one possibleÂ Â  explanation for the human sample efficiencyÂ \nthat needs to be considered is evolution. Evolution has given us a small amountÂ \nof the most useful information possible.Â  For things like vision, hearing, andÂ \nlocomotion, I think there's a prettyÂ Â  strong case that evolution has given us a lot.\nFor example, human dexterity far exceedsâ€¦ I meanÂ Â  robots can become dexterous too if you subjectÂ \nthem to a huge amount of training in simulation.Â  But to train a robot in the real worldÂ \nto quickly pick up a new skill like a person does seems very out of reach.\nHere you could say, \"Oh yeah, locomotion.Â  All our ancestors neededÂ \ngreat locomotion, squirrels. So with locomotion, maybe we'veÂ \ngot some unbelievable prior.\"Â  You could make the same case for vision.\nI believe Yann LeCun made the point that children learn to drive after 10Â \nhours of practice, which is true.Â  But our vision is so good.\nAt least for me,Â Â  I remember myself being a five-year-old.\nI was very excited about cars back then.Â  I'm pretty sure my car recognition was more thanÂ \nadequate for driving already as a five-year-old.Â  You don't get to see thatÂ \nmuch data as a five-year-old.Â  You spend most of your time in your parents'Â \nhouse, so you have very low data diversity. But you could say maybe that's evolution too.\nBut in language and math and coding, probably not.Â  It still seems better than models.\nObviously, models are better than the averageÂ Â  human at language, math, and coding.\nBut are they better thanÂ Â  the average human at learning?\nOh yeah. Oh yeah, absolutely. What I meantÂ Â  to say is that language, math, and codingâ€”andÂ \nespecially math and codingâ€”suggests that whatever it is that makes people good at learning isÂ \nprobably not so much a complicated prior,Â Â  but something more, some fundamental thing.\nI'm not sure I understood. WhyÂ Â  should that be the case?\nSo consider a skill in whichÂ Â  people exhibit some kind of great reliability.\nIf the skill is one that was very useful to ourÂ Â  ancestors for many millions of years, hundredsÂ \nof millions of years, you could argue that maybe humans are good at it because of evolution,Â \nbecause we have a prior, an evolutionary priorÂ Â  that's encoded in some very non-obviousÂ \nway that somehow makes us so good at it.Â  But if people exhibit great ability, reliability,Â \nrobustness, and ability to learn in a domain thatÂ Â  really did not exist until recently, thenÂ \nthis is more an indication that peopleÂ Â  might have just better machine learning, period.\nHow should we think about what that is? What is the ML analogy? There are a couple of interestingÂ \nthings about it. It takes fewer samples. It'sÂ Â  more unsupervised. A child learning to drive aÂ \ncarâ€¦ Children are not learning to drive a car.Â  A teenager learning how to drive a car is notÂ \nexactly getting some prebuilt, verifiable reward.Â  It comes from their interaction withÂ \nthe machine and with the environment.Â Â  It takes much fewer samples. It seemsÂ \nmore unsupervised. It seems more robust? Much more robust. The robustnessÂ \nof people is really staggering. Do you have a unified way of thinking aboutÂ \nwhy all these things are happening at once?Â  What is the ML analogy that couldÂ \nrealize something like this? One of the things that you've been asking about isÂ \nhow can the teenage driver self-correct and learnÂ Â  from their experience without an external teacher?\nThe answer is that they have their value function.Â  They have a general sense which is also,Â \nby the way, extremely robust in people. Whatever the human value function is,Â \nwith a few exceptions around addiction,Â Â  it's actually very, very robust.\nSo for something like a teenagerÂ Â  that's learning to drive, they start to drive, andÂ \nthey already have a sense of how they're drivingÂ Â  immediately, how badly they are, how unconfident.Â \nAnd then they see, \"Okay.\" And then, of course,Â Â  the learning speed of any teenager is so fast.\nAfter 10 hours, you're good to go. It seems like humans have someÂ \nsolution, but I'm curious about how they are doing it and why is it so hard?\nHow do we need to reconceptualize the wayÂ Â  we're training models to makeÂ \nsomething like this possible? That is a great question to ask, and it'sÂ \na question I have a lot of opinions about.Â  But unfortunately, we live in a world whereÂ \nnot all machine learning ideas are discussedÂ Â  freely, and this is one of them.\nThere's probably a way to do it.Â  I think it can be done.\nThe fact that people are like that,Â Â  I think it's a proof that it can be done.\nThere may be another blocker though,Â Â  which is that there is a possibility that theÂ \nhuman neurons do more compute than we think. If that is true, and if that plays an importantÂ \nrole, then things might be more difficult.Â  But regardless, I do think it points toÂ \nthe existence of some machine learningÂ Â  principle that I have opinions on.\nBut unfortunately, circumstancesÂ Â  make it hard to discuss in detail.\nNobody listens to this podcast, Ilya.Â  I'm curious. If you say we are back in an eraÂ \nof research, you were there from 2012 to 2020. What is the vibe now going to be ifÂ \nwe go back to the era of research?Â  For example, even after AlexNet, theÂ \namount of compute that was used toÂ Â  run experiments kept increasing, and theÂ \nsize of frontier systems kept increasing. Do you think now that this era of research willÂ \nstill require tremendous amounts of compute? Do you think it will require going backÂ \ninto the archives and reading old papers? You were at Google and OpenAI and Stanford, theseÂ \nplaces, when there was more of a vibe of research?Â  What kind of things should weÂ \nbe expecting in the community? One consequence of the age of scaling is thatÂ \nscaling sucked out all the air in the room. Because scaling sucked out all the air in theÂ \nroom, everyone started to do the same thing.Â  We got to the point where we areÂ \nin a world where there are moreÂ Â  companies than ideas by quite a bit.\nActually on that, there is this SiliconÂ Â  Valley saying that says that ideasÂ \nare cheap, execution is everything.Â  People say that a lot, and there is truth to that.\nBut then I saw someone say on TwitterÂ Â  something like, \"If ideas are so cheap,Â \nhow come no one's having any ideas?\" And I think it's true too.\nIf you think about research progress in termsÂ Â  of bottlenecks, there are several bottlenecks.\nOne of them is ideas, and one of them is yourÂ Â  ability to bring them to life, whichÂ \nmight be compute but also engineering.Â  If you go back to the '90s, let's say,Â \nyou had people who had pretty good ideas,Â Â  and if they had much larger computers, maybe theyÂ \ncould demonstrate that their ideas were viable. But they could not, so they could onlyÂ \nhave a very, very small demonstrationÂ Â  that did not convince anyone. So theÂ \nbottleneck was compute. Then in theÂ Â  age of scaling, compute has increased a lot.\nOf course, there is a question of how muchÂ Â  compute is needed, but compute is large.\nCompute is large enough such that it's notÂ Â  obvious that you need that much moreÂ \ncompute to prove some idea. I'll give you an analogy. AlexNet was built on two GPUs.\nThat was the total amount of compute used for it.Â  The transformer was built on 8 to 64 GPUs.\nNo single transformer paper experiment usedÂ Â  more than 64 GPUs of 2017, which would beÂ \nlike, what, two GPUs of today? The ResNet,Â Â  right? You could argue that the o1 reasoning wasÂ \nnot the most compute-heavy thing in the world.Â  So for research, you definitely needÂ \nsome amount of compute, but it's far from obvious that you need the absolutelyÂ \nlargest amount of compute ever for research.Â  You might argue, and I think it is true, thatÂ \nif you want to build the absolutely best system then it helps to have much more compute.\nEspecially if everyone is within the sameÂ Â  paradigm, then compute becomesÂ \none of the big differentiators. I'm asking you for the history,Â \nbecause you were actually there.Â  I'm not sure what actually happened.\nIt sounds like it was possible to developÂ Â  these ideas using minimal amounts of compute.\nBut the transformer didn'tÂ Â  immediately become famous.\nIt became the thing everybody startedÂ Â  doing and then started experimenting on top ofÂ \nand building on top of because it was validatedÂ Â  at higher and higher levels of compute.\nCorrect. And if you at SSI have 50 different ideas, howÂ \nwill you know which one is the next transformerÂ Â  and which one is brittle, without having theÂ \nkinds of compute that other frontier labs have? I can comment on that. The shortÂ \ncomment is that you mentioned SSI.Â  Specifically for us, the amount of computeÂ \nthat SSI has for research is really not thatÂ Â  small. I want to explain why. Simple mathÂ \ncan explain why the amount of compute thatÂ Â  we have is comparable for research than one mightÂ \nthink. I'll explain. SSI has raised $3 billion,Â Â  which is a lot by any absolute sense.\nBut you could say, \"Look at the other companies raising much more.\"\nBut a lot of their compute goes for inference.Â  These big numbers, these big loans, it'sÂ \nearmarked for inference. That's number one.Â Â  Number two, if you want to have a productÂ \non which you do inference, you need toÂ Â  have a big staff of engineers, salespeople.\nA lot of the research needs to be dedicated toÂ Â  producing all kinds of product-related features.\nSo then when you look at what's actually left for research, the difference becomes a lot smaller.\nThe other thing is, if you are doing somethingÂ Â  different, do you really need theÂ \nabsolute maximal scale to prove it?Â  I don't think that's true at all.\nI think that in our case, we have sufficientÂ Â  compute to prove, to convince ourselves andÂ \nanyone else, that what we are doing is correct.Â  There have been public estimates that companiesÂ \nlike OpenAI spend on the order of $5-6 billion a year just so far, on experiments.\nThis is separate from the amount ofÂ Â  money they're spending on inference and so forth.\nSo it seems like they're spending more a yearÂ Â  running research experiments thanÂ \nyou guys have in total funding.Â  I think it's a question of what you do with it.\nIt's a question of what you do with it. In their case, in the case of others, thereÂ \nis a lot more demand on the training compute.Â  Thereâ€™s a lot more different work streams, thereÂ \nare different modalities, there is just moreÂ Â  stuff. So it becomes fragmented.\nHow will SSI make money? My answer to this question is something like this.\nRight now, we just focus on the research, and thenÂ Â  the answer to that question will reveal itself.\nI think there will be lots of possible answers.Â  Is SSI's plan still to straightÂ \nshot superintelligence?Â  Maybe. I think that there is merit to it.\nI think there's a lot of merit becauseÂ Â  it's very nice to not be affected byÂ \nthe day-to-day market competition.Â  But I think there are two reasonsÂ \nthat may cause us to change the plan. One is pragmatic, if timelines turnedÂ \nout to be long, which they might.Â  Second, I think there is a lotÂ \nof value in the best and mostÂ Â  powerful AI being out there impacting the world.\nI think this is a meaningfully valuable thing.Â  So then why is your default planÂ \nto straight shot superintelligence?Â  Because it sounds like OpenAI, Anthropic, allÂ \nthese other companies, their explicit thinking is, \"Look, we have weaker and weaker intelligencesÂ \nthat the public can get used to and prepare for.\"Â  Why is it potentially better toÂ \nbuild a superintelligence directly?Â  I'll make the case for and against.\nThe case for is that one of the challengesÂ Â  that people face when they're in the market isÂ \nthat they have to participate in the rat race.Â  The rat race is quite difficult inÂ \nthat it exposes you to difficult trade-offs which you need to make.\nIt is nice to say, \"We'll insulate ourselvesÂ Â  from all this and just focus on the research andÂ \ncome out only when we are ready, and not before.\"Â  But the counterpoint is valid too,Â \nand those are opposing forces.Â  The counterpoint is, \"Hey, it is usefulÂ \nfor the world to see powerful AI.Â  It is useful for the world toÂ \nsee powerful AI because that'sÂ Â  the only way you can communicate it.\"\nWell, I guess not even just that you can communicate the ideaâ€”\nCommunicate the AI,Â Â  not the idea. Communicate the AI.\nWhat do you mean, \"communicate the AI\"?Â  Let's suppose you write an essay about AI, andÂ \nthe essay says, \"AI is going to be this, and AI isÂ Â  going to be that, and it's going to be this.\"\nYou read it and you say, \"Okay,Â Â  this is an interesting essay.\"\nNow suppose you see an AI doing this,Â Â  an AI doing that. It is incomparable. BasicallyÂ \nI think that there is a big benefit from AI being in the public, and that would be aÂ \nreason for us to not be quite straight shot.Â  I guess it's not even that, but I doÂ \nthink that is an important part of it.Â  The other big thing is that I can't think ofÂ \nanother discipline in human engineering andÂ Â  research where the end artifact was madeÂ \nsafer mostly through just thinking aboutÂ Â  how to make it safe, as opposed to,Â \nwhy airplane crashes per mile are so much lower today than they were decades ago.\nWhy is it so much harder to find a bug in LinuxÂ Â  than it would have been decades ago?\nI think it's mostly because theseÂ Â  systems were deployed to the world.\nYou noticed failures, those failuresÂ Â  were corrected and the systems became more robust.\nI'm not sure why AGI and superhuman intelligenceÂ Â  would be any different, especially givenâ€”and IÂ \nhope we're going to get to thisâ€”it seems like the harms of superintelligence are not just aboutÂ \nhaving some malevolent paper clipper out there.Â  But this is a really powerful thing and we don'tÂ \neven know how to conceptualize how people interactÂ Â  with it, what people will do with it.\nHaving gradual access to it seems like a better way to maybe spread out the impactÂ \nof it and to help people prepare for it.Â  Well I think on this point, even in the straightÂ \nshot scenario, you would still do a gradualÂ Â  release of it, thatâ€™s how I would imagine it.\nGradualism would be an inherentÂ Â  component of any plan.\nIt's just a question of what is the firstÂ Â  thing that you get out of the door. That's numberÂ \none. Number two, I believe you have advocated for continual learning more than other people,Â \nand I actually think that this is an importantÂ Â  and correct thing. Here is why. I'll give youÂ \nanother example of how language affects thinking.Â  In this case, it will be two words thatÂ \nhave shaped everyone's thinking, I maintain. First word: AGI. Second word: pre-training.Â \nLet me explain. The term AGI, why does thisÂ Â  term exist? It's a very particular term. WhyÂ \ndoes it exist? There's a reason. The reasonÂ Â  that the term AGI exists is, in my opinion, notÂ \nso much because it's a very important, essentialÂ Â  descriptor of some end state of intelligence, butÂ \nbecause it is a reaction to a different term thatÂ Â  existed, and the term is narrow AI.\nIf you go back to ancient history of gameplay and AI, of checkers AI, chessÂ \nAI, computer games AI, everyone would say,Â Â  look at this narrow intelligence.\nSure, the chess AI can beat Kasparov,Â Â  but it can't do anything else.\nIt is so narrow, artificial narrow intelligence.Â  So in response, as a reaction to this,Â \nsome people said, this is not good. ItÂ Â  is so narrow. What we need is general AI,Â \nan AI that can just do all the things. That term just got a lot of traction.\nThe second thing that got a lot of tractionÂ Â  is pre-training, specificallyÂ \nthe recipe of pre-training.Â  I think the way people do RL now is maybeÂ \nundoing the conceptual imprint of pre-training.Â Â  But pre-training had this property. YouÂ \ndo more pre-training and the model getsÂ Â  better at everything, more or less uniformly.Â \nGeneral AI. Pre-training gives AGI. But the thing that happened with AGI and pre-trainingÂ \nis that in some sense they overshot the target. If you think about the term \"AGI\",Â \nespecially in the context of pre-training,Â Â  you will realize that a human being is not an AGI.\nYes, there is definitely a foundation of skills,Â Â  but a human being lacks aÂ \nhuge amount of knowledge.Â  Instead, we rely on continual learning.\nSo when you think about, \"Okay,Â Â  so let's suppose that we achieve success and weÂ \nproduce some kind of safe superintelligence.\"Â  The question is, how do you define it?\nWhere on the curve of continual learning is it going to be?\nI produce a superintelligentÂ Â  15-year-old that's very eager to go.\nThey don't know very much at all,Â Â  a great student, very eager.\nYou go and be a programmer,Â Â  you go and be a doctor, go and learn.\nSo you could imagine that the deploymentÂ Â  itself will involve some kind ofÂ \na learning trial-and-error period.Â  It's a process, as opposed toÂ \nyou dropping the finished thing.Â  I see. You're suggesting that the thingÂ \nyou're pointing out with superintelligence is not some finished mind which knows howÂ \nto do every single job in the economy.Â  Because the way, say, the original OpenAI charterÂ \nor whatever defines AGI is like, it can do everyÂ Â  single job, every single thing a human can do.\nYou're proposing instead a mind which canÂ Â  learn to do every single job,Â \nand that is superintelligence.Â  Yes.\nBut once you have the learning algorithm,Â Â  it gets deployed into the world the same wayÂ \na human laborer might join an organization. Exactly.\nIt seems like one of these two thingsÂ Â  might happen, maybe neither of these happens.\nOne, this super-efficient learning algorithmÂ Â  becomes superhuman, becomes as goodÂ \nas you and potentially even better,Â Â  at the task of ML research.\nAs a result the algorithmÂ Â  itself becomes more and more superhuman.\nThe other is, even if that doesn't happen,Â Â  if you have a single modelâ€”this is explicitlyÂ \nyour visionâ€”where instances of a model which are deployed through the economy doingÂ \ndifferent jobs, learning how to do those jobs,Â Â  continually learning on the job, picking upÂ \nall the skills that any human could pick up,Â Â  but picking them all up at the same time,Â \nand then amalgamating their learnings,Â Â  you basically have a model which functionallyÂ \nbecomes superintelligent even without any sortÂ Â  of recursive self-improvement in software.\nBecause you now have one model that can do every single job in the economy and humansÂ \ncan't merge our minds in the same way.Â  So do you expect some sort of intelligenceÂ \nexplosion from broad deployment?Â  I think that it is likely that weÂ \nwill have rapid economic growth.Â  I think with broad deployment, there are twoÂ \narguments you could make which are conflicting.Â  One is that once indeed you get to a point whereÂ \nyou have an AI that can learn to do things quickly and you have many of them, then there will beÂ \na strong force to deploy them in the economyÂ Â  unless there will be some kind of a regulationÂ \nthat stops it, which by the way there might be.Â  But the idea of very rapidÂ \neconomic growth for some time,Â Â  I think itâ€™s very possible from broad deployment.\nThe question is how rapid it's going to be.Â  I think this is hard to know because on theÂ \none hand you have this very efficient worker. On the other hand, the world is justÂ \nreally big and there's a lot of stuff,Â Â  and that stuff moves at a different speed.\nBut then on the other hand, now the AI couldâ€¦Â Â  So I think very rapid economic growth is possible.\nWe will see all kinds of things like differentÂ Â  countries with different rules and theÂ \nones which have the friendlier rules, theÂ Â  economic growth will be faster. Hard to predict.\nIt seems to me that this is a very precariousÂ Â  situation to be in.\nIn the limit, we know that this should be possible.\nIf you have something that is as goodÂ Â  as a human at learning, but which can merge itsÂ \nbrainsâ€”merge different instances in a way thatÂ Â  humans can't mergeâ€”already, this seems likeÂ \na thing that should physically be possible.Â  Humans are possible, digitalÂ \ncomputers are possible.Â  You just need both of thoseÂ \ncombined to produce this thing.Â  It also seems this kind ofÂ \nthing is extremely powerful. Economic growth is one way to put it.\nA Dyson sphere is a lot of economic growth.Â  But another way to put it is that you will have,Â \nin potentially a very short period of time... You hire people at SSI, and in sixÂ \nmonths, they're net productive, probably.Â  A human learns really fast, and this thingÂ \nis becoming smarter and smarter very fast.Â  How do you think about making that go well?\nWhy is SSI positioned to do that well?Â  What is SSI's plan there, isÂ \nbasically what I'm trying to ask. One of the ways in which my thinking has beenÂ \nchanging is that I now place more importance onÂ Â  AI being deployed incrementally and in advance.\nOne very difficult thing about AI is that we areÂ Â  talking about systems that don't yetÂ \nexist and it's hard to imagine them. I think that one of the things that's happening isÂ \nthat in practice, it's very hard to feel the AGI.Â  It's very hard to feel the AGI.\nWe can talk about it, but imagineÂ Â  having a conversation about how it isÂ \nlike to be old when you're old and frail.Â  You can have a conversation, you can try toÂ \nimagine it, but it's just hard, and you comeÂ Â  back to reality where that's not the case.\nI think that a lot of the issues around AGI and its future power stem from the factÂ \nthat it's very difficult to imagine.Â  Future AI is going to be different. It's goingÂ \nto be powerful. Indeed, the whole problem,Â Â  what is the problem of AI and AGI?\nThe whole problem is the power.Â  The whole problem is the power.\nWhen the power is really big,Â Â  what's going to happen?\nOne of the ways in which I'veÂ Â  changed my mind over the past yearâ€”and thatÂ \nchange of mind, I'll hedge a little bit, may back-propagate into the plans of our companyâ€”isÂ \nthat if it's hard to imagine, what do you do?Â  Youâ€™ve got to be showing the thing.\nYouâ€™ve got to be showing the thing.Â  I maintain that most people who work on AI alsoÂ \ncan't imagine it because it's too different fromÂ Â  what people see on a day-to-day basis.\nI do maintain, here's something whichÂ Â  I predict will happen. This is a prediction.Â \nI maintain that as AI becomes more powerful, people will change their behaviors.\nWe will see all kinds of unprecedentedÂ Â  things which are not happening right now. Iâ€™llÂ \ngive some examples. I think for better or worse,Â Â  the frontier companies will play a very importantÂ \nrole in what happens, as will the government.Â  The kind of things that I thinkÂ \nyou'll see, which you see theÂ Â  beginnings of, are companies that are fierceÂ \ncompetitors starting to collaborate on AI safety. You may have seen OpenAI and Anthropic doingÂ \na first small step, but that did not exist.Â  That's something which I predicted inÂ \none of my talks about three years ago,Â Â  that such a thing will happen.\nI also maintain that as AI continuesÂ Â  to become more powerful, more visiblyÂ \npowerful, there will also be a desire fromÂ Â  governments and the public to do something.\nI think this is a very important force,Â Â  of showing the AI. That's number one.Â \nNumber two, okay, so the AI is being built. What needs to be done? One thing thatÂ \nI maintain that will happen is that right now,Â Â  people who are working on AI, I maintain that theÂ \nAI doesn't feel powerful because of its mistakes.Â  I do think that at some point the AIÂ \nwill start to feel powerful actually.Â  I think when that happens, we will see a bigÂ \nchange in the way all AI companies approachÂ Â  safety. They'll become much more paranoid.Â \nI say this as a prediction that we will see happen. We'll see if I'm right. But I thinkÂ \nthis is something that will happen because theyÂ Â  will see the AI becoming more powerful.\nEverything that's happening right now,Â Â  I maintain, is because people look at today'sÂ \nAI and it's hard to imagine the future AI.Â  There is a third thing which needs to happen.\nI'm talking about it in broader terms,Â Â  not just from the perspective of SSIÂ \nbecause you asked me about our company.Â  The question is, what shouldÂ \nthe companies aspire to build? What should they aspire to build?\nThere has been one big idea that everyone has been locked into, which isÂ \nthe self-improving AI. Why did it happen?Â Â  Because there are fewer ideas than companies.\nBut I maintain that there is something that'sÂ Â  better to build, and I thinkÂ \nthat everyone will want that.Â  It's the AI that's robustly aligned toÂ \ncare about sentient life specifically.Â  I think in particular, there's a case toÂ \nbe made that it will be easier to buildÂ Â  an AI that cares about sentient life thanÂ \nan AI that cares about human life alone, because the AI itself will be sentient.\nAnd if you think about things like mirrorÂ Â  neurons and human empathy for animals, which youÂ \nmight argue it's not big enough, but it exists.Â  I think it's an emergent property fromÂ \nthe fact that we model others with theÂ Â  same circuit that we use to model ourselves,Â \nbecause that's the most efficient thing to do.Â  So even if you got an AI to care aboutÂ \nsentient beingsâ€”and it's not actually clear to me that that's what youÂ \nshould try to do if you solvedÂ Â  alignmentâ€”it would still be the caseÂ \nthat most sentient beings will be AIs.Â  There will be trillions,Â \neventually quadrillions, of AIs.Â  Humans will be a very smallÂ \nfraction of sentient beings.Â  So it's not clear to me if the goal is some kindÂ \nof human control over this future civilization,Â Â  that this is the best criterion.\nIt's true. It's possible it's not the best criterion. I'll say two things. NumberÂ \none, care for sentient life, I think there isÂ Â  merit to it. It should be considered. I think itÂ \nwould be helpful if there was some kind of shortÂ Â  list of ideas that the companies, when they areÂ \nin this situation, could use. Thatâ€™s number two.Â Â  Number three, I think it would be reallyÂ \nmaterially helpful if the power of theÂ Â  most powerful superintelligence was somehow cappedÂ \nbecause it would address a lot of these concerns. The question of how to do it, I'm not sure, but IÂ \nthink that would be materially helpful when you'reÂ Â  talking about really, really powerful systems.\nBefore we continue the alignment discussion,Â Â  I want to double-click on that.\nHow much room is there at the top?Â  How do you think about superintelligence?\nDo you think, using this learning efficiency idea,Â Â  maybe it is just extremely fast atÂ \nlearning new skills or new knowledge? Does it just have a bigger pool of strategies?\nIs there a single cohesive \"it\" in theÂ Â  center that's more powerful or bigger?\nIf so, do you imagine that this will beÂ Â  sort of godlike in comparison to the rest of humanÂ \ncivilization, or does it just feel like anotherÂ Â  agent, or another cluster of agents?\nThis is an area where differentÂ Â  people have different intuitions.\nI think it will be very powerful, for sure. What I think is most likely to happenÂ \nis that there will be multiple suchÂ Â  AIs being created roughly at the same time.\nI think that if the cluster is big enoughâ€”likeÂ Â  if the cluster is literally continent-sizedâ€”thatÂ \nthing could be really powerful, indeed.Â  If you literally have a continent-sizedÂ \ncluster, those AIs can be very powerful. All I can tell you is that if you'reÂ \ntalking about extremely powerful AIs,Â Â  truly dramatically powerful, it would be nice ifÂ \nthey could be restrained in some ways or if thereÂ Â  were some kind of agreement or something.\nWhat is the concern of superintelligence?Â  What is one way to explain the concern?\nIf you imagine a system that is sufficientlyÂ Â  powerful, really sufficiently powerfulâ€”and youÂ \ncould say you need to do something sensible like care for sentient life in a very single-mindedÂ \nwayâ€”we might not like the results. That's reallyÂ Â  what it is. Maybe, by the way, the answer isÂ \nthat you do not build an RL agent in the usualÂ Â  sense. I'll point several things out. IÂ \nthink human beings are semi-RL agents.Â  We pursue a reward, and then the emotionsÂ \nor whatever make us tire out of theÂ Â  reward and we pursue a different reward.\nThe market is a very short-sighted kind of agent. Evolution is the same. EvolutionÂ \nis very intelligent in some ways,Â Â  but very dumb in other ways.\nThe government has been designedÂ Â  to be a never-ending fight betweenÂ \nthree parts, which has an effect.Â  So I think things like this.\nAnother thing that makes this discussionÂ Â  difficult is that we are talking about systemsÂ \nthat don't exist, that we don't know how to build.Â  Thatâ€™s the other thing andÂ \nthatâ€™s actually my belief. I think what people are doing right nowÂ \nwill go some distance and then peter out.Â  It will continue to improve,Â \nbut it will also not be \"it\".Â  The \"It\" we don't know how to build, andÂ \na lot hinges on understanding reliableÂ Â  generalization. Iâ€™ll say another thing.Â \nOne of the things that you could say aboutÂ Â  what causes alignment to be difficult is thatÂ \nyour ability to learn human values is fragile.Â  Then your ability to optimize them is fragile.\nYou actually learn to optimize them. And can't you say, \"Are these not allÂ \ninstances of unreliable generalization?\"Â  Why is it that human beings appearÂ \nto generalize so much better?Â  What if generalization was much better?\nWhat would happen in this case? What wouldÂ Â  be the effect? But those questionsÂ \nare right now still unanswerable.Â  How does one think about whatÂ \nAI going well looks like?Â  You've scoped out how AI might evolve.\nWe'll have these sort of continual learning agents. AI will be very powerful.Â \nMaybe there will be many different AIs.Â  How do you think about lots of continent-sizedÂ \ncompute intelligences going around? How dangerousÂ Â  is that? How do we make that less dangerous?\nAnd how do we do that in a way that protects anÂ Â  equilibrium where there might be misalignedÂ \nAIs out there and bad actors out there?Â  Hereâ€™s one reason why I liked \"AIÂ \nthat cares for sentient life\". We can debate on whether it's good or bad.\nBut if the first N of these dramaticÂ Â  systems do care for, love, humanityÂ \nor something, care for sentient life,Â Â  obviously this also needs to be achieved. ThisÂ \nneeds to be achieved. So if this is achievedÂ Â  by the first N of those systems, then I canÂ \nsee it go well, at least for quite some time.Â  Then there is the question ofÂ \nwhat happens in the long run.Â  How do you achieve a long-run equilibrium?\nI think that there, there is an answer as well. I don't like this answer, butÂ \nit needs to be considered. In the long run, you might say, \"Okay, ifÂ \nyou have a world where powerful AIs exist,Â Â  in the short term, you could sayÂ \nyou have universal high income.Â  You have universal high incomeÂ \nand we're all doing well.\"Â  But what do the Buddhists say? \"Change is theÂ \nonly constant.\" Things change. There is someÂ Â  kind of government, political structure thing, andÂ \nit changes because these things have a shelf life.Â  Some new government thing comes up andÂ \nit functions, and then after some time it stops functioning.\nThat's something thatÂ Â  we see happening all the time.\nSo I think for the long-run equilibrium,Â Â  one approach is that you could say maybe everyÂ \nperson will have an AI that will do their bidding,Â Â  and that's good.\nIf that could beÂ Â  maintained indefinitely, that's true.\nBut the downside with that is then the AIÂ Â  goes and earns money for the person and advocatesÂ \nfor their needs in the political sphere, and maybe then writes a little report saying, \"Okay,Â \nhere's what I've done, here's the situation,\"Â Â  and the person says, \"Great, keep it up.\"\nBut the person is no longer a participant.Â  Then you can say that's aÂ \nprecarious place to be in. I'm going to preface by saying I don'tÂ \nlike this solution, but it is a solution. The solution is if people becomeÂ \npart-AI with some kind of Neuralink++.Â  Because what will happen as a result isÂ \nthat now the AI understands something,Â Â  and we understand it too, because now theÂ \nunderstanding is transmitted wholesale.Â  So now if the AI is in some situation, youÂ \nare involved in that situation yourself fully.Â  I think this is the answer to the equilibrium.\nI wonder if the fact that emotions which were developed millionsâ€”or in many cases, billionsâ€”ofÂ \nyears ago in a totally different environment areÂ Â  still guiding our actions so stronglyÂ \nis an example of alignment success.Â  To spell out what I meanâ€”I donâ€™t knowÂ \nwhether itâ€™s more accurate to call itÂ Â  a value function or reward functionâ€”but theÂ \nbrainstem has a directive where it's saying,Â Â  \"Mate with somebody who's more successful.\"\nThe cortex is the part that understands what success means in the modern context.\nBut the brainstem is able to align the cortexÂ Â  and say, \"However you recognize success to beâ€”andÂ \nIâ€™m not smart enough to understand what that isâ€”Â Â  you're still going to pursue this directive.\"\nI think there's a more general point.Â  I think it's actually really mysteriousÂ \nhow evolution encodes high-level desires.Â  It's pretty easy to understand howÂ \nevolution would endow us with the desire for food that smells good because smellÂ \nis a chemical, so just pursue that chemical.Â  It's very easy to imagineÂ \nevolution doing that thing.Â  But evolution also has endowedÂ \nus with all these social desires.Â  We really care about beingÂ \nseen positively by society.Â  We care about being in good standing.\nAll these social intuitions that we have,Â Â  I feel strongly that they're baked in.\nI don't know how evolution did itÂ Â  because it's a high-level conceptÂ \nthat's represented in the brain. Letâ€™s say you care about some social thing,Â \nit's not a low-level signal like smell.Â  It's not something for which there is a sensor.\nThe brain needs to do a lot of processing toÂ Â  piece together lots of bits of informationÂ \nto understand what's going on socially.Â  Somehow evolution said, \"That's what you shouldÂ \ncare about.\" How did it do it? It did it quickly,Â Â  too. All these sophisticated social things that weÂ \ncare about, I think they evolved pretty recently. Evolution had an easy timeÂ \nhard-coding this high-level desire. I'm unaware of a goodÂ \nhypothesis for how it's done.Â  I had some ideas I was kicking around,Â \nbut none of them are satisfying.Â  What's especially impressive is it was desireÂ \nthat you learned in your lifetime, it makes senseÂ Â  because your brain is intelligent.\nIt makes sense why you wouldÂ Â  be able to learn intelligent desires.\nMaybe this is not your point, but one wayÂ Â  to understand it is that the desire is built intoÂ \nthe genome, and the genome is not intelligent. But you're somehow able to describe this feature.\nIt's not even clear how you define that feature,Â Â  and you can build it into the genes.\nEssentially, or maybe I'll put it differently.Â  If you think about the tools thatÂ \nare available to the genome, it says,Â Â  \"Okay, here's a recipe for building a brain.\"\nYou could say, \"Here is a recipe for connectingÂ Â  the dopamine neurons to the smell sensor.\"\nAnd if the smell is a certain kind of good smell, you want to eat that.\nI could imagine the genome doing that.Â  I'm claiming that it is harder to imagine.\nIt's harder to imagine the genome sayingÂ Â  you should care about some complicated computationÂ \nthat your entire brain, a big chunk of your brain,Â Â  does. That's all I'm claiming. I can tellÂ \nyou a speculation of how it could be done.Â  Let me offer a speculation, and I'll explainÂ \nwhy the speculation is probably false. So the brain has brain regions. We haveÂ \nour cortex. It has all those brain regions.Â  The cortex is uniform, but the brainÂ \nregions and the neurons in the cortexÂ Â  kind of speak to their neighbors mostly.\nThat explains why you get brain regions.Â  Because if you want to do some kind ofÂ \nspeech processing, all the neurons thatÂ Â  do speech need to talk to each other.\nAnd because neurons can only speak toÂ Â  their nearby neighbors, for theÂ \nmost part, it has to be a region. All the regions are mostly located inÂ \nthe same place from person to person.Â  So maybe evolution hard-codedÂ \nliterally a location on the brain.Â  So it says, \"Oh, when the GPS coordinatesÂ \nof the brain such and such, when that fires,Â Â  that's what you should care about.\"\nMaybe that's what evolution did becauseÂ Â  that would be within the toolkit of evolution.\nYeah, although there are examples where,Â Â  for example, people who are born blind have thatÂ \narea of their cortex adopted by another sense. I have no idea, but I'd be surprised if theÂ \ndesires or the reward functions which require aÂ Â  visual signal no longer worked for people who haveÂ \ntheir different areas of their cortex co-opted.Â  For example, if you no longer have vision, canÂ \nyou still feel the sense that I want peopleÂ Â  around me to like me and so forth, whichÂ \nusually there are also visual cues for.Â  I fully agree with that. I think there's anÂ \neven stronger counterargument to this theory. There are people who get half ofÂ \ntheir brains removed in childhood,Â Â  and they still have all their brain regions.\nBut they all somehow move to just one hemisphere,Â Â  which suggests that the brain regions,Â \ntheir location is not fixed and soÂ Â  that theory is not true.\nIt would have been coolÂ Â  if it was true, but it's not.\nSo I think that's a mystery.Â Â  But it's an interesting mystery. The fact isÂ \nthat somehow evolution was able to endow us to care about social stuff very, very reliably.\nEven people who have all kinds of strange mentalÂ Â  conditions and deficiencies and emotionalÂ \nproblems tend to care about this also. What is SSI planning on doing differently?\nPresumably your plan is to be one of theÂ Â  frontier companies when this time arrives.\nPresumably you started SSI because you're like,Â Â  \"I think I have a way of approaching howÂ \nto do this safely in a way that the otherÂ Â  companies don't.\" What is that difference?\nThe way I would describe it is that thereÂ Â  are some ideas that I think are promising andÂ \nI want to investigate them and see if they are indeed promising or not. It's really thatÂ \nsimple. It's an attempt. If the ideas turn outÂ Â  to be correctâ€”these ideas that we discussedÂ \naround understanding generalizationâ€”then IÂ Â  think we will have something worthy.\nWill they turn out to be correct? WeÂ Â  are doing research. We are squarely an \"age ofÂ \nresearch\" company. We are making progress. We'veÂ Â  actually made quite good progress over the pastÂ \nyear, but we need to keep making more progress, more research. That's how I see it. I see itÂ \nas an attempt to be a voice and a participant. Your cofounder and previous CEO left to go toÂ \nMeta recently, and people have asked, \"Well,Â Â  if there were a lot of breakthroughs beingÂ \nmade, that seems like a thing that shouldÂ Â  have been unlikely.\" I wonder how you respond.\nFor this, I will simply remind a few facts thatÂ Â  may have been forgotten.\nI think these facts whichÂ Â  provide the context explain the situation.\nThe context was that we were fundraising at a $32 billion valuation, and then Meta cameÂ \nin and offered to acquire us, and I said no.Â  But my former cofounder in some sense said yes.\nAs a result, he also was able to enjoy a lot ofÂ Â  near-term liquidity, and he was theÂ \nonly person from SSI to join Meta.Â  It sounds like SSI's plan is to be a companyÂ \nthat is at the frontier when you get to thisÂ Â  very important period in human historyÂ \nwhere you have superhuman intelligence. You have these ideas about how toÂ \nmake superhuman intelligence go well.Â  But other companies willÂ \nbe trying their own ideas.Â  What distinguishes SSI's approachÂ \nto making superintelligence go well?Â  The main thing that distinguishesÂ \nSSI is its technical approach.Â  We have a different technical approach thatÂ \nI think is worthy and we are pursuing it.Â  I maintain that in the end thereÂ \nwill be a convergence of strategies. I think there will be a convergence of strategiesÂ \nwhere at some point, as AI becomes more powerful,Â Â  it's going to become more or less clearerÂ \nto everyone what the strategy should be.Â  It should be something like, you need to findÂ \nsome way to talk to each other and you wantÂ Â  your first actual real superintelligent AI toÂ \nbe aligned and somehow care for sentient life,Â Â  care for people, democratic, oneÂ \nof those, some combination thereof. I think this is the conditionÂ \nthat everyone should strive for.Â  That's what SSI is striving for.\nI think that this time, if not already,Â Â  all the other companies will realize thatÂ \nthey're striving towards the same thing.Â Â  We'll see. I think that the world willÂ \ntruly change as AI becomes more powerful. I think things will be really different andÂ \npeople will be acting really differently.Â  Speaking of forecasts, what are yourÂ \nforecasts to this system you're describing,Â Â  which can learn as well as a human andÂ \nsubsequently, as a result, become superhuman?Â  I think like 5 to 20.\n5 to 20 years?Â  Mhm.\nI just wantÂ Â  to unroll how you might see the world coming.\nIt's like, we have a couple more years whereÂ Â  these other companies are continuingÂ \nthe current approach and it stalls out. \"Stalls out\" here meaning they earn no moreÂ \nthan low hundreds of billions in revenue?Â  How do you think about what stalling out means?\nI think stalling out will look likeâ€¦it willÂ Â  all look very similar amongÂ \nall the different companies.Â  It could be something like this.\nI'm not sure because I think even with stalling out, I think theseÂ \ncompanies could make a stupendous revenue.Â  Maybe not profits because they will needÂ \nto work hard to differentiate each otherÂ Â  from themselves, but revenue definitely.\nBut something in your model implies thatÂ Â  when the correct solution does emerge, thereÂ \nwill be convergence between all the companies.Â  I'm curious why you think that's the case.\nI was talking more about convergenceÂ Â  on their alignment strategies.\nI think eventual convergence on the technical approach is probably goingÂ \nto happen as well, but I was alludingÂ Â  to convergence to the alignment strategies.\nWhat exactly is the thing that should be done?Â  I just want to better understandÂ \nhow you see the future unrolling.Â  Currently, we have these different companies, andÂ \nyou expect their approach to continue generatingÂ Â  revenue but not get to this human-like learner.\nSo now we have these different forks of companies. We have you, we have Thinking Machines,Â \nthere's a bunch of other labs.Â  Maybe one of them figuresÂ \nout the correct approach.Â  But then the release of their product makesÂ \nit clear to other people how to do this thing.Â  I think it won't be clear how to do it, butÂ \nit will be clear that something different isÂ Â  possible, and that is information.\nPeople will then be tryingÂ Â  to figure out how that works.\nI do think though that one of the things not addressed here, not discussed, is that with eachÂ \nincrease in the AI's capabilities, I think thereÂ Â  will be some kind of changes, but I don't knowÂ \nexactly which ones, in how things are being done. I think it's going to be important, yetÂ \nI can't spell out what that is exactly. By default, you would expect the company thatÂ \nhas that model to be getting all these gainsÂ Â  because they have the model that has the skillsÂ \nand knowledge that it's building up in the world.Â  What is the reason to think that the benefitsÂ \nof that would be widely distributed and notÂ Â  just end up at whatever model company getsÂ \nthis continuous learning loop going first? Here is what I think is going to happen.\nNumber one, let's look at how things haveÂ Â  gone so far with the AIs of the past.\nOne company produced an advance and theÂ Â  other company scrambled and produced some similarÂ \nthings after some amount of time and they startedÂ Â  to compete in the market and push the prices down.\nSo I think from the market perspective,Â Â  something similar will happen there as well.\nWe are talking about the good world, by the way. What's the good world? Itâ€™s where we have theseÂ \npowerful human-like learners that are alsoâ€¦ ByÂ Â  the way, maybe there's another thing we haven'tÂ \ndiscussed on the spec of the superintelligentÂ Â  AI that I think is worth considering.\nItâ€™s that you make it narrow, it canÂ Â  be useful and narrow at the same time.\nYou can have lots of narrow superintelligent AIs.Â  But suppose you have many of them and youÂ \nhave some company that's producing a lot of profits from it.\nThen you have anotherÂ Â  company that comes in and starts to compete.\nThe way the competition is going to work isÂ Â  through specialization. Competition lovesÂ \nspecialization. You see it in the market,Â Â  you see it in evolution as well.\nYou're going to have lots of differentÂ Â  niches and you're going to have lots of differentÂ \ncompanies who are occupying different niches.Â  In this world we might say one AI companyÂ \nis really quite a bit better at some area of really complicated economic activity and aÂ \ndifferent company is better at another area.Â  And the third company isÂ \nreally good at litigation.Â  Isn't this contradicted by what human-likeÂ \nlearning implies? Itâ€™s that it can learnâ€¦Â  It can, but you have accumulatedÂ \nlearning. You have a big investment.Â Â  You spent a lot of compute to become really,Â \nreally good, really phenomenal at this thing.Â  Someone else spent a huge amountÂ \nof compute and a huge amount of experience to get really good at some other thing.\nYou apply a lot of human learning to get there,Â Â  but now you are at this high point whereÂ \nsomeone else would say, \"Look, I don't wantÂ Â  to start learning what you've learned.\"\nI guess that would require many differentÂ Â  companies to begin at the human-like continualÂ \nlearning agent at the same time so that theyÂ Â  can start their different treeÂ \nsearch in different branches. But if one company gets that agent first, or getsÂ \nthat learner first, it does then seem likeâ€¦ Well, if you just think about every single job inÂ \nthe economy, having an instance learning eachÂ Â  one seems tractable for a company.\nThat's a valid argument. My strongÂ Â  intuition is that it's not how it's going to go.\nThe argument says it will go this way, but myÂ Â  strong intuition is that it will not go this way.\nIn theory, there is no difference between theoryÂ Â  and practice. In practice, there is. IÂ \nthink that's going to be one of those. A lot of people's models of recursiveÂ \nself-improvement literally, explicitly stateÂ Â  we will have a million Ilyas in a server that areÂ \ncoming up with different ideas, and this will leadÂ Â  to a superintelligence emerging very fast.\nDo you have some intuition about howÂ Â  parallelizable the thing you are doing is?\nWhat are the gains from making copies of Ilya?Â  I donâ€™t know. I think there'll definitely beÂ \ndiminishing returns because you want people who think differently rather than the same.\nIf there were literal copies of me, I'm not sureÂ Â  how much more incremental value you'd get.\nPeople who think differently,Â Â  that's what you want.\nWhy is it that if you lookÂ Â  at different models, even released by totallyÂ \ndifferent companies trained on potentiallyÂ Â  non-overlapping datasets, it's actuallyÂ \ncrazy how similar LLMs are to each other?Â  Maybe the datasets are not asÂ \nnon-overlapping as it seems. But thereâ€™s some sense in which evenÂ \nif an individual human might be lessÂ Â  productive than the future AI, maybe thereâ€™sÂ \nsomething to the fact that human teams haveÂ Â  more diversity than teams of AIs might have.\nHow do we elicit meaningful diversity among AIs?Â  I think just raising the temperatureÂ \njust results in gibberish.Â  You want something more like different scientistsÂ \nhave different prejudices or different ideas.Â  How do you get that kind ofÂ \ndiversity among AI agents? So the reason there has been no diversity,Â \nI believe, is because of pre-training.Â  All the pre-trained models are pretty much theÂ \nsame because they pre-train on the same data.Â  Now RL and post-training is whereÂ \nsome differentiation starts to emergeÂ Â  because different people comeÂ \nup with different RL training. I've heard you hint in the pastÂ \nabout self-play as a way to eitherÂ Â  get data or match agents to other agents ofÂ \nequivalent intelligence to kick off learning.Â  How should we think about why there are no publicÂ \nproposals of this kind of thing working with LLMs?Â  I would say there are two things to say.\nThe reason why I thought self-play wasÂ Â  interesting is because it offered a way toÂ \ncreate models using compute only, without data. If you think that data is the ultimate bottleneck,Â \nthen using compute only is very interesting.Â  So that's what makes it interesting.\nThe thing is that self-play, at least theÂ Â  way it was done in the pastâ€”when you have agentsÂ \nwhich somehow compete with each otherâ€”it's onlyÂ Â  good for developing a certain set of skills. ItÂ \nis too narrow. It's only good for negotiation,Â Â  conflict, certain social skills,Â \nstrategizing, that kind of stuff. If you care about those skills,Â \nthen self-play will be useful.Â  Actually, I think that self-play did findÂ \na home, but just in a different form. So things like debate, prover-verifier, youÂ \nhave some kind of an LLM-as-a-Judge which isÂ Â  also incentivized to find mistakes in your work.\nYou could say this is not exactly self-play,Â Â  but this is a related adversarialÂ \nsetup that people are doing, I believe.Â  Really self-play is a special case ofÂ \nmore general competition between agents.Â  The natural response to competitionÂ \nis to try to be different. So if you were to put multiple agents togetherÂ \nand you tell them, \"You all need to work on someÂ Â  problem and you are an agent and you're inspectingÂ \nwhat everyone else is working,\" theyâ€™re going toÂ Â  say, \"Well, if they're already taking thisÂ \napproach, it's not clear I should pursue it.Â Â  I should pursue something differentiated.\" So IÂ \nthink something like this could also create anÂ Â  incentive for a diversity of approaches.\nFinal question: What is research taste? You're obviously the person inÂ \nthe world who is considered toÂ Â  have the best taste in doing research in AI.\nYou were the co-author on the biggest thingsÂ Â  that have happened in the history of deepÂ \nlearning, from AlexNet to GPT-3 to so on.Â  What is it, how do you characterizeÂ \nhow you come up with these ideas?Â  I can comment on this for myself.\nI think different people do it differently.Â  One thing that guides me personally is anÂ \naesthetic of how AI should be, by thinking about how people are, but thinking correctly.\nIt's very easy to think about how people areÂ Â  incorrectly, but what does it mean to thinkÂ \nabout people correctly? I'll give you someÂ Â  examples. The idea of the artificial neuronÂ \nis directly inspired by the brain, and it's aÂ Â  great idea. Why? Because you say the brain hasÂ \nall these different organs, it has the folds,Â Â  but the folds probably don't matter.\nWhy do we think that the neurons matter? Because there are many of them.\nIt kind of feels right, so you want the neuron.Â  You want some local learning rule that willÂ \nchange the connections between the neurons.Â  It feels plausible that the brain does it.\nThe idea of the distributed representation.Â  The idea that the brain respondsÂ \nto experience therefore our neuralÂ Â  net should learn from experience.\nThe brain learns from experience, the neural net should learn from experience.\nYou kind of ask yourself, is something fundamentalÂ Â  or not fundamental? How things should be.Â \nI think that's been guiding me a fair bit,Â Â  thinking from multiple angles and lookingÂ \nfor almost beauty, beauty and simplicity.Â  Ugliness, there's no room for ugliness.\nIt's beauty, simplicity, elegance,Â Â  correct inspiration from the brain.\nAll of those things need toÂ Â  be present at the same time.\nThe more they are present, the more confident you can be in a top-down belief.\nThe top-down belief is the thing that sustainsÂ Â  you when the experiments contradict you.\nBecause if you trust the data all the time,Â Â  well sometimes you can be doing theÂ \ncorrect thing but there's a bug.Â  But you don't know that there is a bug.\nHow can you tell that there is a bug?Â  How do you know if you should keep debugging orÂ \nyou conclude it's the wrong direction? It's the top-down. You can say things have to be this way.\nSomething like this has to work,Â Â  therefore weâ€™ve got to keep going.\nThat's the top-down, and it's based on thisÂ Â  multifaceted beauty and inspiration by the brain.\nAlright, we'll leave it there.Â  Thank you so much.\nIlya, thank you so much.Â  Alright. Appreciate it.\nThat was great.Â  Yeah, I enjoyed it.\nYes, me too."
    }
  ],
  "temperature": 0.1
}